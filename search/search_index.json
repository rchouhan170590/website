{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Full Stack Deep Learning Our mission is to help you go from a promising ML experiment to a shipped product, with real-world impact. Current Course We are teaching a major update of the course Spring 2021 as an official UC Berkeley course and as an online course, with all lectures and labs available for free. \ud83d\ude80Spring 2021 Online Course\ud83d\ude80 About this course There are many great courses to learn how to train deep neural networks. However, training the model is just one part of shipping a deep learning project. This course teaches full-stack production deep learning: Formulating the problem and estimating project cost Finding, cleaning, labeling, and augmenting data Picking the right framework and compute infrastructure Troubleshooting training and ensuring reproducibility Deploying the model at scale Who is this for The course is aimed at people who already know the basics of deep learning and want to understand the rest of the process of creating production deep learning systems. You will get the most out of this course if you have: At least one-year experience programming in Python. At least one deep learning course (at a university or online). Experience with code versioning, Unix environments, and software engineering. While we cover the basics of deep learning (backpropagation, convolutional neural networks, recurrent neural networks, transformers, etc), we expect these lectures to be mostly review. Instructors Sergey Karayev is Head of STEM AI at Turnitin. He co-founded Gradescope after getting a PhD in Computer Vision at UC Berkeley. Josh Tobin is Founder and CEO of a stealth startup. He worked as a Research Scientist at OpenAI after getting a PhD in Robotics at UC Berkeley. Pieter Abbeel is Professor at UC Berkeley. He co-founded Covariant.ai, Berkeley Open Arms, and Gradescope. Our March 2019 Bootcamp Course Offerings \ud83d\ude80Spring 2021\ud83d\ude80 Online Course Spring 2021 CSE 194-080 - UC Berkeley Undergraduate course Spring 2020 CSEP 590C - University of Washington Professional Master's Program course \u2728Fall 2019\u2728 nicely formatted videos and notes from our weekend bootcamp March 2019 raw videos from our bootcamp August 2018 bootcamp","title":"Home"},{"location":"#current-course","text":"We are teaching a major update of the course Spring 2021 as an official UC Berkeley course and as an online course, with all lectures and labs available for free. \ud83d\ude80Spring 2021 Online Course\ud83d\ude80","title":"Current Course"},{"location":"#about-this-course","text":"There are many great courses to learn how to train deep neural networks. However, training the model is just one part of shipping a deep learning project. This course teaches full-stack production deep learning: Formulating the problem and estimating project cost Finding, cleaning, labeling, and augmenting data Picking the right framework and compute infrastructure Troubleshooting training and ensuring reproducibility Deploying the model at scale","title":"About this course"},{"location":"#who-is-this-for","text":"The course is aimed at people who already know the basics of deep learning and want to understand the rest of the process of creating production deep learning systems. You will get the most out of this course if you have: At least one-year experience programming in Python. At least one deep learning course (at a university or online). Experience with code versioning, Unix environments, and software engineering. While we cover the basics of deep learning (backpropagation, convolutional neural networks, recurrent neural networks, transformers, etc), we expect these lectures to be mostly review.","title":"Who is this for"},{"location":"#instructors","text":"Sergey Karayev is Head of STEM AI at Turnitin. He co-founded Gradescope after getting a PhD in Computer Vision at UC Berkeley. Josh Tobin is Founder and CEO of a stealth startup. He worked as a Research Scientist at OpenAI after getting a PhD in Robotics at UC Berkeley. Pieter Abbeel is Professor at UC Berkeley. He co-founded Covariant.ai, Berkeley Open Arms, and Gradescope. Our March 2019 Bootcamp","title":"Instructors"},{"location":"#course-offerings","text":"\ud83d\ude80Spring 2021\ud83d\ude80 Online Course Spring 2021 CSE 194-080 - UC Berkeley Undergraduate course Spring 2020 CSEP 590C - University of Washington Professional Master's Program course \u2728Fall 2019\u2728 nicely formatted videos and notes from our weekend bootcamp March 2019 raw videos from our bootcamp August 2018 bootcamp","title":"Course Offerings"},{"location":"spring2021/","text":"Full Stack Deep Learning - Spring 2021 We've updated and improved our materials and can't wait to share them with you! Every Monday, we will post videos of our lectures and lab sessions. You can follow along on our Twitter or YouTube , or sign up via email below. Synchronous Online Course We also offered a paid option for those who wanted weekly assignments, capstone project, Slack discussion, and certificate of completion. This synchronous option is now full, but you can enter your email above to be the first to hear about future offerings. Those who are enrolled, please see details . Week 1: Fundamentals The week of February 1, we do a blitz review of the fundamentals of deep learning, and introduce the codebase we will be working on in labs for the remainder of the class. Lecture 1: DL Fundamentals Notebook: Coding a neural net from scratch Lab 1: Setup and Intro Reading: How the backpropagation algorithm works Week 2: CNNs The week of February 8, we cover CNNs and Computer Vision Applications, and introduce a CNN in lab. Lecture 2A: CNNs Lecture 2B: Computer Vision Applications Lab 2: CNNs Reading: A brief introduction to Neural Style Transfer Improving the way neural networks learn Week 3: RNNs The week of February 15, we cover RNNs and applications in Natural Language Processing, and start doing sequence processing in lab. Lecture 3: RNNs Lab 3: RNNs Reading: The Unreasonable Effectiveness of Recurrent Neural Networks Attention Craving RNNS: Building Up To Transformer Networks Week 4: Transformers The week of February 22, we talk about the successes of transfer learning and the Transformer architecture, and start using it in lab. Lecture 4: Transfer Learning and Transformers Lab 4: Transformers Reading: Transformers from Scratch Week 5: ML Projects The week of March 1, our synchronous online course begins with the first \"Full Stack\" lecture: Setting up ML Projects. Lecture 5: Setting up ML Projects (\ud83d\udc48 with detailed notes) Reading: Rules of Machine Learning Those in the syncronous online course will have their first weekly assignment: Assignment 1 , available on Gradescope. Week 6: Infra & Tooling The week of March 7, we tour the landscape of infrastructure and tooling for deep learning. Lecture 6: Infrastructure & Tooling (\ud83d\udc48 with detailed notes) Reading: Machine Learning: The High-Interest Credit Card of Technical Debt Those in the syncronous online course will have to work on Assignment 2 . Week 7: Troubleshooting The week of March 14, we talk about how to best troubleshoot training. In lab, we learn to manage experiments. Lecture 7: Troubleshooting DNNs (\ud83d\udc48 with detailed notes) Lab 5: Experiment Management Reading: Why is machine learning hard? Those in the syncronous online course will have to work on Assignment 3 . Week 8: Data The week of March 21, we talk about Data Management, and label some data in lab. Lecture 8: Data Management (\ud83d\udc48 with detailed notes) Lab 6: Data Labeling Reading: Emerging architectures for modern data infrastructure Those in the syncronous online course will have to work on Assignment 4 . Week 9: Ethics The week of March 28, we discuss ethical considerations. In lab, we move from lines to paragraphs. Lecture 9: AI Ethics Lab 7: Line Detection or Paragraph Recognition Those in the synchronous online course will have to submit their project proposals . Week 10: Testing The week of April 5, we talk about Testing and Explainability, and set up Continuous Integration in lab. Lecture 10: Testing & Explainability Lab 8: Testing & CI Those in the synchronous online course will work on their projects. Week 11: Deployment The week of April 12, we cover Deployment and Monitoring, and deploy our model to AWS Lambda in lab. Lecture 11: Deployment & Monitoring Lab 9: Web Deployment Those in the synchronous online course will work on their projects. Week 12: Research The week of April 19, we talk research, and set up robust monitoring for our model. Lecture 12: Research Directions Lab 10: Monitoring Those in the synchronous online course will work on their projects. Week 13: Teams The week of April 26, we discuss ML roles and team structures, as well as big companies vs startups. Lecture 13: ML Teams & Startups Those in the synchronous online course will submit 5-minute videos of their projects and associated write-ups. Week 14: Projects The week of May 3, we watch the best course project videos together, and give out awards. There are rumors of a fun conference in the air, too... Other Resources Fast.ai is a great free two-course sequence aimed at first getting hackers to train state-of-the-art models as quickly as possible, and only afterward delving into how things work under the hood. Highly recommended for anyone. Dive Into Deep Learning is a great free textbook with Jupyter notebooks for every part of deep learning. NYU\u2019s Deep Learning course has excellent PyTorch breakdowns of everything important going on in deep learning. Stanford\u2019s ML Systems Design course has lectures that parallel those in this course. The Batch by Andrew Ng is a great weekly update on progress in the deep learning world. /r/MachineLearning/ is the best community for staying up to date with the latest developments.","title":"Spring 2021 Schedule"},{"location":"spring2021/#full-stack-deep-learning-spring-2021","text":"We've updated and improved our materials and can't wait to share them with you! Every Monday, we will post videos of our lectures and lab sessions. You can follow along on our Twitter or YouTube , or sign up via email below. Synchronous Online Course We also offered a paid option for those who wanted weekly assignments, capstone project, Slack discussion, and certificate of completion. This synchronous option is now full, but you can enter your email above to be the first to hear about future offerings. Those who are enrolled, please see details .","title":"Full Stack Deep Learning - Spring 2021"},{"location":"spring2021/#week-1-fundamentals","text":"The week of February 1, we do a blitz review of the fundamentals of deep learning, and introduce the codebase we will be working on in labs for the remainder of the class. Lecture 1: DL Fundamentals Notebook: Coding a neural net from scratch Lab 1: Setup and Intro Reading: How the backpropagation algorithm works","title":"Week 1: Fundamentals"},{"location":"spring2021/#week-2-cnns","text":"The week of February 8, we cover CNNs and Computer Vision Applications, and introduce a CNN in lab. Lecture 2A: CNNs Lecture 2B: Computer Vision Applications Lab 2: CNNs Reading: A brief introduction to Neural Style Transfer Improving the way neural networks learn","title":"Week 2: CNNs"},{"location":"spring2021/#week-3-rnns","text":"The week of February 15, we cover RNNs and applications in Natural Language Processing, and start doing sequence processing in lab. Lecture 3: RNNs Lab 3: RNNs Reading: The Unreasonable Effectiveness of Recurrent Neural Networks Attention Craving RNNS: Building Up To Transformer Networks","title":"Week 3: RNNs"},{"location":"spring2021/#week-4-transformers","text":"The week of February 22, we talk about the successes of transfer learning and the Transformer architecture, and start using it in lab. Lecture 4: Transfer Learning and Transformers Lab 4: Transformers Reading: Transformers from Scratch","title":"Week 4: Transformers"},{"location":"spring2021/#week-5-ml-projects","text":"The week of March 1, our synchronous online course begins with the first \"Full Stack\" lecture: Setting up ML Projects. Lecture 5: Setting up ML Projects (\ud83d\udc48 with detailed notes) Reading: Rules of Machine Learning Those in the syncronous online course will have their first weekly assignment: Assignment 1 , available on Gradescope.","title":"Week 5: ML Projects"},{"location":"spring2021/#week-6-infra-tooling","text":"The week of March 7, we tour the landscape of infrastructure and tooling for deep learning. Lecture 6: Infrastructure & Tooling (\ud83d\udc48 with detailed notes) Reading: Machine Learning: The High-Interest Credit Card of Technical Debt Those in the syncronous online course will have to work on Assignment 2 .","title":"Week 6: Infra &amp; Tooling"},{"location":"spring2021/#week-7-troubleshooting","text":"The week of March 14, we talk about how to best troubleshoot training. In lab, we learn to manage experiments. Lecture 7: Troubleshooting DNNs (\ud83d\udc48 with detailed notes) Lab 5: Experiment Management Reading: Why is machine learning hard? Those in the syncronous online course will have to work on Assignment 3 .","title":"Week 7: Troubleshooting"},{"location":"spring2021/#week-8-data","text":"The week of March 21, we talk about Data Management, and label some data in lab. Lecture 8: Data Management (\ud83d\udc48 with detailed notes) Lab 6: Data Labeling Reading: Emerging architectures for modern data infrastructure Those in the syncronous online course will have to work on Assignment 4 .","title":"Week 8: Data"},{"location":"spring2021/#week-9-ethics","text":"The week of March 28, we discuss ethical considerations. In lab, we move from lines to paragraphs. Lecture 9: AI Ethics Lab 7: Line Detection or Paragraph Recognition Those in the synchronous online course will have to submit their project proposals .","title":"Week 9: Ethics"},{"location":"spring2021/#week-10-testing","text":"The week of April 5, we talk about Testing and Explainability, and set up Continuous Integration in lab. Lecture 10: Testing & Explainability Lab 8: Testing & CI Those in the synchronous online course will work on their projects.","title":"Week 10: Testing"},{"location":"spring2021/#week-11-deployment","text":"The week of April 12, we cover Deployment and Monitoring, and deploy our model to AWS Lambda in lab. Lecture 11: Deployment & Monitoring Lab 9: Web Deployment Those in the synchronous online course will work on their projects.","title":"Week 11: Deployment"},{"location":"spring2021/#week-12-research","text":"The week of April 19, we talk research, and set up robust monitoring for our model. Lecture 12: Research Directions Lab 10: Monitoring Those in the synchronous online course will work on their projects.","title":"Week 12: Research"},{"location":"spring2021/#week-13-teams","text":"The week of April 26, we discuss ML roles and team structures, as well as big companies vs startups. Lecture 13: ML Teams & Startups Those in the synchronous online course will submit 5-minute videos of their projects and associated write-ups.","title":"Week 13: Teams"},{"location":"spring2021/#week-14-projects","text":"The week of May 3, we watch the best course project videos together, and give out awards. There are rumors of a fun conference in the air, too...","title":"Week 14: Projects"},{"location":"spring2021/#other-resources","text":"Fast.ai is a great free two-course sequence aimed at first getting hackers to train state-of-the-art models as quickly as possible, and only afterward delving into how things work under the hood. Highly recommended for anyone. Dive Into Deep Learning is a great free textbook with Jupyter notebooks for every part of deep learning. NYU\u2019s Deep Learning course has excellent PyTorch breakdowns of everything important going on in deep learning. Stanford\u2019s ML Systems Design course has lectures that parallel those in this course. The Batch by Andrew Ng is a great weekly update on progress in the deep learning world. /r/MachineLearning/ is the best community for staying up to date with the latest developments.","title":"Other Resources"},{"location":"spring2021/lab-1/","text":"Lab 1: Setup and Introduction Video In this video, we introduce the lab throughout the course. We formulate the problem, provide the codebase structure, and train a simple Multilayer Perceptron on the MNIST dataset. 4:11 - Understand the problem and path to solution 5:54 - Set up the computing environment 12:54 - Review the codebase 24:55 - Train the MLP model on MNIST Slides PDF Download Follow Along GitHub Readme","title":"Lab 1: Setup and Introduction"},{"location":"spring2021/lab-1/#lab-1-setup-and-introduction","text":"","title":"Lab 1: Setup and Introduction"},{"location":"spring2021/lab-1/#video","text":"In this video, we introduce the lab throughout the course. We formulate the problem, provide the codebase structure, and train a simple Multilayer Perceptron on the MNIST dataset. 4:11 - Understand the problem and path to solution 5:54 - Set up the computing environment 12:54 - Review the codebase 24:55 - Train the MLP model on MNIST","title":"Video"},{"location":"spring2021/lab-1/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lab-1/#follow-along","text":"GitHub Readme","title":"Follow Along"},{"location":"spring2021/lab-10/","text":"\u2728Lab 10: Monitoring\u2728 Video No slides Follow along Readme Notes TODO","title":"\u2728Lab 10: Monitoring\u2728"},{"location":"spring2021/lab-10/#lab-10-monitoring","text":"","title":"\u2728Lab 10: Monitoring\u2728"},{"location":"spring2021/lab-10/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-10/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-10/#notes","text":"TODO","title":"Notes"},{"location":"spring2021/lab-2/","text":"Lab 2: CNNs and Synthetic Data Video No slides. Follow Along GitHub Readme In this lab, you train a single-line ConvNet predictor on the EMNIST dataset and then synthetically generate your own data. 00:00 - Introduction 05:23 - Look at the EMNIST dataset 09:52 - Train a base ConvNet model 12:43 - Examine the ConvNet code 17:33 - Lab 2 homework 19:35 - Make a synthetic dataset of EMNIST lines","title":"Lab 2: CNNs and Synthetic Data"},{"location":"spring2021/lab-2/#lab-2-cnns-and-synthetic-data","text":"","title":"Lab 2: CNNs and Synthetic Data"},{"location":"spring2021/lab-2/#video","text":"No slides.","title":"Video"},{"location":"spring2021/lab-2/#follow-along","text":"GitHub Readme In this lab, you train a single-line ConvNet predictor on the EMNIST dataset and then synthetically generate your own data. 00:00 - Introduction 05:23 - Look at the EMNIST dataset 09:52 - Train a base ConvNet model 12:43 - Examine the ConvNet code 17:33 - Lab 2 homework 19:35 - Make a synthetic dataset of EMNIST lines","title":"Follow Along"},{"location":"spring2021/lab-3/","text":"Lab 3: RNNs Video No slides Follow along Readme Notes 00:00 - Introduction. 01:59 - Introduce LineCNNSimple, a model that can read multiple characters in an image. 15:52 - Make this model more efficient with LineCNN, which uses a fully convolutional network. 18:18 - Upgrade the model further into LitModelCTC, which uses a CTC (Connectionist Temporal Classification) loss. 23:29 - Finalize your model, LineCNNLSTM, by adding an LSTM layer on top of CNN. 27:34 - Lab 3 homework.","title":"Lab 3: RNNs"},{"location":"spring2021/lab-3/#lab-3-rnns","text":"","title":"Lab 3: RNNs"},{"location":"spring2021/lab-3/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-3/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-3/#notes","text":"00:00 - Introduction. 01:59 - Introduce LineCNNSimple, a model that can read multiple characters in an image. 15:52 - Make this model more efficient with LineCNN, which uses a fully convolutional network. 18:18 - Upgrade the model further into LitModelCTC, which uses a CTC (Connectionist Temporal Classification) loss. 23:29 - Finalize your model, LineCNNLSTM, by adding an LSTM layer on top of CNN. 27:34 - Lab 3 homework.","title":"Notes"},{"location":"spring2021/lab-4/","text":"Lab 4: Transformers Video No slides Follow along Readme Notes In this lab, you use the LineCNN + LSTM model with CTC loss from lab 3 as an \"encoder\" of the image, and then send it through Transformer decoder layers. 00:00 - Introduction 01:43 - LineCNNTransformer class 04:50 - TransformerLitModel 06:51 - Code to make predictions 08:50 - Training guidelines","title":"Lab 4: Transformers"},{"location":"spring2021/lab-4/#lab-4-transformers","text":"","title":"Lab 4: Transformers"},{"location":"spring2021/lab-4/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-4/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-4/#notes","text":"In this lab, you use the LineCNN + LSTM model with CTC loss from lab 3 as an \"encoder\" of the image, and then send it through Transformer decoder layers. 00:00 - Introduction 01:43 - LineCNNTransformer class 04:50 - TransformerLitModel 06:51 - Code to make predictions 08:50 - Training guidelines","title":"Notes"},{"location":"spring2021/lab-5/","text":"Lab 5: Experiment Management Video No slides Follow along Readme Notes In this lab, we'll use Weights and Biases to manage experiments for our handwriting recognition model. 00:00 - Introduction 00:56 - IAMLines Dataset 05:29 - Make EMNISTLines more like IAMLines 09:57 - Set up Weights and Biases 13:42 - Run experiments on Weights and Biases 22:58 - Configure W&B sweeps to search for hyper-parameters","title":"Lab 5: Experiment Management"},{"location":"spring2021/lab-5/#lab-5-experiment-management","text":"","title":"Lab 5: Experiment Management"},{"location":"spring2021/lab-5/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-5/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-5/#notes","text":"In this lab, we'll use Weights and Biases to manage experiments for our handwriting recognition model. 00:00 - Introduction 00:56 - IAMLines Dataset 05:29 - Make EMNISTLines more like IAMLines 09:57 - Set up Weights and Biases 13:42 - Run experiments on Weights and Biases 22:58 - Configure W&B sweeps to search for hyper-parameters","title":"Notes"},{"location":"spring2021/lab-6/","text":"\u2728Lab 6: Data Labeling\u2728 Video Follow along Readme","title":"\u2728Lab 6: Data Labeling\u2728"},{"location":"spring2021/lab-6/#lab-6-data-labeling","text":"","title":"\u2728Lab 6: Data Labeling\u2728"},{"location":"spring2021/lab-6/#video","text":"","title":"Video"},{"location":"spring2021/lab-6/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-7/","text":"\u2728Lab 7\u2728 Video No slides Follow along Readme Notes TODO","title":"\u2728Lab 7\u2728"},{"location":"spring2021/lab-7/#lab-7","text":"","title":"\u2728Lab 7\u2728"},{"location":"spring2021/lab-7/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-7/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-7/#notes","text":"TODO","title":"Notes"},{"location":"spring2021/lab-8/","text":"\u2728Lab 8: Testing & CI\u2728 Video No slides Follow along Readme Notes TODO","title":"\u2728Lab 8: Testing & CI\u2728"},{"location":"spring2021/lab-8/#lab-8-testing-ci","text":"","title":"\u2728Lab 8: Testing &amp; CI\u2728"},{"location":"spring2021/lab-8/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-8/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-8/#notes","text":"TODO","title":"Notes"},{"location":"spring2021/lab-9/","text":"\u2728Lab 9: Web Deployment\u2728 Video No slides Follow along Readme Notes TODO","title":"\u2728Lab 9: Web Deployment\u2728"},{"location":"spring2021/lab-9/#lab-9-web-deployment","text":"","title":"\u2728Lab 9: Web Deployment\u2728"},{"location":"spring2021/lab-9/#video","text":"No slides","title":"Video"},{"location":"spring2021/lab-9/#follow-along","text":"Readme","title":"Follow along"},{"location":"spring2021/lab-9/#notes","text":"TODO","title":"Notes"},{"location":"spring2021/lecture-1/","text":"Lecture 1: DL Fundamentals Video Slides PDF Download Notes In this video, we discuss the fundamentals of deep learning. We will cover artificial neural networks, the universal approximation theorem, three major types of learning problems, the empirical risk minimization problem, the idea behind gradient descent, the practice of back-propagation, the core neural architectures, and the rise of GPUs. This should be a review for most of you; if not, then briefly go through this online book -neuralnetworksanddeeplearning.com. 1:25\u200b - Neural Networks 6:48\u200b - Universality 8:48\u200b - Learning Problems 16:17\u200b - Empirical Risk Minimization / Loss Functions 19:55\u200b - Gradient Descent 23:57\u200b - Backpropagation / Automatic Differentiation 26:09\u200b - Architectural Considerations 29:01\u200b - CUDA / Cores of Compute","title":"Lecture 1: DL Fundamentals"},{"location":"spring2021/lecture-1/#lecture-1-dl-fundamentals","text":"","title":"Lecture 1: DL Fundamentals"},{"location":"spring2021/lecture-1/#video","text":"","title":"Video"},{"location":"spring2021/lecture-1/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-1/#notes","text":"In this video, we discuss the fundamentals of deep learning. We will cover artificial neural networks, the universal approximation theorem, three major types of learning problems, the empirical risk minimization problem, the idea behind gradient descent, the practice of back-propagation, the core neural architectures, and the rise of GPUs. This should be a review for most of you; if not, then briefly go through this online book -neuralnetworksanddeeplearning.com. 1:25\u200b - Neural Networks 6:48\u200b - Universality 8:48\u200b - Learning Problems 16:17\u200b - Empirical Risk Minimization / Loss Functions 19:55\u200b - Gradient Descent 23:57\u200b - Backpropagation / Automatic Differentiation 26:09\u200b - Architectural Considerations 29:01\u200b - CUDA / Cores of Compute","title":"Notes"},{"location":"spring2021/lecture-10/","text":"\u2728Lecture 10: Testing & Explainability\u2728 Video Slides PDF Download Notes","title":"\u2728Lecture 10: Testing & Explainability\u2728"},{"location":"spring2021/lecture-10/#lecture-10-testing-explainability","text":"","title":"\u2728Lecture 10: Testing &amp; Explainability\u2728"},{"location":"spring2021/lecture-10/#video","text":"","title":"Video"},{"location":"spring2021/lecture-10/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-10/#notes","text":"","title":"Notes"},{"location":"spring2021/lecture-11/","text":"\u2728Lecture 11: Deployment & Monitoring\u2728 Video Slides PDF Download Notes","title":"\u2728Lecture 11: Deployment & Monitoring\u2728"},{"location":"spring2021/lecture-11/#lecture-11-deployment-monitoring","text":"","title":"\u2728Lecture 11: Deployment &amp; Monitoring\u2728"},{"location":"spring2021/lecture-11/#video","text":"","title":"Video"},{"location":"spring2021/lecture-11/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-11/#notes","text":"","title":"Notes"},{"location":"spring2021/lecture-12/","text":"\u2728Lecture 12: Research Directions\u2728 Video Slides PDF Download Notes","title":"\u2728Lecture 12: Research Directions\u2728"},{"location":"spring2021/lecture-12/#lecture-12-research-directions","text":"","title":"\u2728Lecture 12: Research Directions\u2728"},{"location":"spring2021/lecture-12/#video","text":"","title":"Video"},{"location":"spring2021/lecture-12/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-12/#notes","text":"","title":"Notes"},{"location":"spring2021/lecture-13/","text":"\u2728Lecture 13: ML Teams and Startups\u2728 Video Slides PDF Download Notes","title":"\u2728Lecture 13: ML Teams and Startups\u2728"},{"location":"spring2021/lecture-13/#lecture-13-ml-teams-and-startups","text":"","title":"\u2728Lecture 13: ML Teams and Startups\u2728"},{"location":"spring2021/lecture-13/#video","text":"","title":"Video"},{"location":"spring2021/lecture-13/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-13/#notes","text":"","title":"Notes"},{"location":"spring2021/lecture-2a/","text":"Lecture 2A: CNNs Video Slides PDF Download Notes In this video, we first review convolution operation, the most basic property of Convolutional Neural Networks. Then, we look at other important operations for ConvNets. Finally, we transition to looking at a classic ConvNet architecture called LeNet. 00:00 - Introduction 01:08 - Convolutional Filters 07:10 - Filter Stacks and ConvNets 11:25 - Strides and Padding 14:35 - Filter Math 21:44 - Convolution Implementation Notes 24:04 - Increasing the Receptive Field with Dilated Convolutions 27:30 - Decreasing the Tensor Size with Pooling and 1x1-Convolutions 30:54 - LeNet Architecture","title":"Lecture 2A: CNNs"},{"location":"spring2021/lecture-2a/#lecture-2a-cnns","text":"","title":"Lecture 2A: CNNs"},{"location":"spring2021/lecture-2a/#video","text":"","title":"Video"},{"location":"spring2021/lecture-2a/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-2a/#notes","text":"In this video, we first review convolution operation, the most basic property of Convolutional Neural Networks. Then, we look at other important operations for ConvNets. Finally, we transition to looking at a classic ConvNet architecture called LeNet. 00:00 - Introduction 01:08 - Convolutional Filters 07:10 - Filter Stacks and ConvNets 11:25 - Strides and Padding 14:35 - Filter Math 21:44 - Convolution Implementation Notes 24:04 - Increasing the Receptive Field with Dilated Convolutions 27:30 - Decreasing the Tensor Size with Pooling and 1x1-Convolutions 30:54 - LeNet Architecture","title":"Notes"},{"location":"spring2021/lecture-2b/","text":"Lecture 2B: Computer Vision Video Slides PDF Download Notes In this video, we will review notable applications of deep learning in computer vision. First, we will tour some ConvNet architectures. Then, we will talk about localization, detection, and segmentation problems. We will conclude with more advanced methods. Learn more at this website: https://paperswithcode.com/area/computer-vision 00:00 - Introduction 02:51 - AlexNet 05:09 - ZFNet 06:54 - VGGNet 09:06 - GoogLeNet 11:57 - ResNet 15:15 - SqueezeNet 17:05 - Architecture Comparisons 20:00 - Localization, Detection, and Segmentation Tasks 24:00 - Overfeat, YOLO, and SSD Methods 28:01 - Region Proposal Methods (R-CNN, Faster R-CNN, Mask R-CNN, U-Net) 34:33 - Advanced Tasks (3D Shape Inference, Face Landmark Recognition, and Pose Estimation) 37:00 - Adversarial Attacks 40:56 - Style Transfer","title":"Lecture 2B: Computer Vision"},{"location":"spring2021/lecture-2b/#lecture-2b-computer-vision","text":"","title":"Lecture 2B: Computer Vision"},{"location":"spring2021/lecture-2b/#video","text":"","title":"Video"},{"location":"spring2021/lecture-2b/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-2b/#notes","text":"In this video, we will review notable applications of deep learning in computer vision. First, we will tour some ConvNet architectures. Then, we will talk about localization, detection, and segmentation problems. We will conclude with more advanced methods. Learn more at this website: https://paperswithcode.com/area/computer-vision 00:00 - Introduction 02:51 - AlexNet 05:09 - ZFNet 06:54 - VGGNet 09:06 - GoogLeNet 11:57 - ResNet 15:15 - SqueezeNet 17:05 - Architecture Comparisons 20:00 - Localization, Detection, and Segmentation Tasks 24:00 - Overfeat, YOLO, and SSD Methods 28:01 - Region Proposal Methods (R-CNN, Faster R-CNN, Mask R-CNN, U-Net) 34:33 - Advanced Tasks (3D Shape Inference, Face Landmark Recognition, and Pose Estimation) 37:00 - Adversarial Attacks 40:56 - Style Transfer","title":"Notes"},{"location":"spring2021/lecture-3/","text":"Lecture 3: RNNs Video Slides PDF Download Notes 00:00 - Introduction 01:34 - Sequence Problems 06:28 - Review of RNNs 22:00 - Vanishing Gradient Issue 27:52 - LSTMs and Its Variants 34:10 - Bidirectionality and Attention from Google's Neural Machine Translation 46:38 - CTC Loss 52:12 - Pros and Cons of Encoder-Decoder LSTM Architectures 54:55 - WaveNet","title":"Lecture 3: RNNs"},{"location":"spring2021/lecture-3/#lecture-3-rnns","text":"","title":"Lecture 3: RNNs"},{"location":"spring2021/lecture-3/#video","text":"","title":"Video"},{"location":"spring2021/lecture-3/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-3/#notes","text":"00:00 - Introduction 01:34 - Sequence Problems 06:28 - Review of RNNs 22:00 - Vanishing Gradient Issue 27:52 - LSTMs and Its Variants 34:10 - Bidirectionality and Attention from Google's Neural Machine Translation 46:38 - CTC Loss 52:12 - Pros and Cons of Encoder-Decoder LSTM Architectures 54:55 - WaveNet","title":"Notes"},{"location":"spring2021/lecture-4/","text":"Lecture 4: Transformers Video Slides PDF Download Notes In this video, you will learn about the origin of transfer learning in computer vision, its application in NLP in the form of embedding, NLP's ImageNet moment, and the Transformers model families. 00:00 - Introduction 00:42 - Transfer Learning in Computer Vision 04:00 - Embeddings and Language Models 10:09 - NLP's ImageNet moment: ELMO and ULMFit on datasets like SQuAD, SNLI, and GLUE 16:49 - Rise of Transformers 18:20 - Attention in Detail: (Masked) Self-Attention, Positional Encoding, and Layer Normalization 27:33 - Transformers Variants: BERT, GPT/GPT-2/GPT-3, DistillBERT, T5, etc. 36:20 - GPT3 Demos 42:53 - Future Directions","title":"Lecture 4: Transformers"},{"location":"spring2021/lecture-4/#lecture-4-transformers","text":"","title":"Lecture 4: Transformers"},{"location":"spring2021/lecture-4/#video","text":"","title":"Video"},{"location":"spring2021/lecture-4/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-4/#notes","text":"In this video, you will learn about the origin of transfer learning in computer vision, its application in NLP in the form of embedding, NLP's ImageNet moment, and the Transformers model families. 00:00 - Introduction 00:42 - Transfer Learning in Computer Vision 04:00 - Embeddings and Language Models 10:09 - NLP's ImageNet moment: ELMO and ULMFit on datasets like SQuAD, SNLI, and GLUE 16:49 - Rise of Transformers 18:20 - Attention in Detail: (Masked) Self-Attention, Positional Encoding, and Layer Normalization 27:33 - Transformers Variants: BERT, GPT/GPT-2/GPT-3, DistillBERT, T5, etc. 36:20 - GPT3 Demos 42:53 - Future Directions","title":"Notes"},{"location":"spring2021/lecture-5/","text":"Lecture 5: ML Projects Learn how to set up Machine Learning projects like a pro. This includes an understanding of the ML lifecycle, an acute mind of the feasibility and impact, an awareness of the project archetypes, and an obsession with metrics and baselines. Video Slides PDF Download Detailed Notes By James Le and Vishnu Rachakonda 1 - Why Do ML Projects Fail? Based on a report from TechRepublic a few years back, despite increased interest in adopting machine learning (ML) in the enterprise, 85% of machine learning projects ultimately fail to deliver on their intended promises to business. Failure can happen for many reasons; however, a few glaring dangers will cause any AI project to crash and burn. ML is still very much a research endeavor. Therefore it is very challenging to aim for a 100% success rate. Many ML projects are technically infeasible or poorly scoped. Many ML projects never leap production, thus getting stuck at the prototype phase. Many ML projects have unclear success criteria because of a lack of understanding of the value proposition. Many ML projects are poorly managed because of a lack of interest from leadership. 2 - Lifecycle It\u2019s essential to understand what constitutes all of the activities in a machine learning project. Typically speaking, there are four major phases: Planning and Project Setup : At this phase, we want to decide the problem to work on, determine the requirements and goals, figure out how to allocate resources properly, consider the ethical implications, etc. Data Collection and Labeling : At this phase, we want to collect training data and potentially annotate them with ground truth, depending on the specific sources where they come from. We may find that it\u2019s too hard to get the data, or it might be easier to label for a different task. If that\u2019s the case, go back to phase 1. Model Training and Model Debugging : At this phase, we want to implement baseline models quickly, find and reproduce state-of-the-art methods for the problem domain, debug our implementation, and improve the model performance for specific tasks. We may realize that we need to collect more data or that labeling is unreliable (thus, go back to phase 2). Or we may recognize that the task is too challenging and there is a tradeoff between project requirements (thus, go back to phase 1). Model Deploying and Model Testing : At this phase, we want to pilot the model in a constrained environment (i.e., in the lab), write tests to prevent regressions, and roll the model into production. We may see that the model doesn\u2019t work well in the lab, so we want to keep improving the model\u2019s accuracy (thus, go back to phase 3). Or we may want to fix the mismatch between training data and production data by collecting more data and mining hard cases (thus go back to phase 2). Or we may find out that the metric picked doesn\u2019t actually drive downstream user behavior, and/or performance in the real world isn\u2019t great. In such situations, we want to revisit the projects\u2019 metrics and requirements (thus, go back to phase 1). Besides the per-project activities mentioned above, there are two other things that any ML team will need to solve across any projects they get involved with: (1) building the team and hiring people; and (2) setting up infrastructure and tooling to build ML systems repeatedly and at scale. Additionally, it might be useful to understand state-of-the-art results in your application domain so that you know what\u2019s possible and what to try next. 3 - Prioritizing Projects To prioritize projects to work on, you want to find high-impact problems and assess the potential costs associated with them. The picture below shows a general framework that encourages us to target projects with high impact and high feasibility. High Impact There are no silver bullets to find high-impact ML problems to work on, but here are a few useful mental models: Where can you take advantage of cheap prediction? Where is there friction in your product? Where can you automate complicated manual processes? What are other people doing? Cheap Prediction In the book \u201c Prediction Machines ,\u201d the authors (Ajay Agrawal, Joshua Gans, and Avi Goldfarb) come up with an excellent mental model on the economics of Artificial Intelligence: As AI reduces the cost of prediction and prediction is central for decision making, cheap predictions would be universal for problems across business domains . Therefore, you should look for projects where cheap predictions will have a huge business impact. Product Needs Another lens is to think about what your product needs. In the article \u201c Three Principles for Designing ML-Powered Products ,\u201d the Spotify Design team emphasizes the importance of building ML from a product perspective and looking for parts of the product experience with high friction . Automating those parts is exactly where there is a lot of impact for ML to make your business better. ML Strength In his popular blog post \u201c Software 2.0 ,\u201d Andrej Karpathy contrasts software 1.0 (which are traditional programs with explicit instructions) and software 2.0 (where humans specify goals, while the algorithm searches for a program that works). Software 2.0 programmers work with datasets, which get compiled via optimization\u200a\u2014\u200awhich works better, more general, and less computationally expensive. Therefore, you should look for complicated rule-based software where we can learn the rules instead of programming them. Inspiration From Others Instead of reinventing the wheel, you can look at what other companies are doing. In particular, check out papers from large frontier organizations (Google, Facebook, Nvidia, Netflix, etc.) and blog posts from top earlier-stage companies (Uber, Lyft, Spotify, Stripe, etc.). Here is a list of excellent ML use cases to check out (credit to Chip Huyen\u2019s ML Systems Design Lecture 2 Note ): Human-Centric Machine Learning Infrastructure at Netflix (Ville Tuulos, InfoQ 2019) 2020 state of enterprise machine learning (Algorithmia, 2020) Using Machine Learning to Predict Value of Homes On Airbnb (Robert Chang, Airbnb Engineering & Data Science, 2017) Using Machine Learning to Improve Streaming Quality at Netflix (Chaitanya Ekanadham, Netflix Technology Blog, 2018) 150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com (Bernardi et al., KDD, 2019) How we grew from 0 to 4 million women on our fashion app, with a vertical machine learning approach (Gabriel Aldamiz, HackerNoon, 2018) Machine Learning-Powered Search Ranking of Airbnb Experiences (Mihajlo Grbovic, Airbnb Engineering & Data Science, 2019) From shallow to deep learning in fraud (Hao Yi Ong, Lyft Engineering, 2018) Space, Time and Groceries (Jeremy Stanley, Tech at Instacart, 2017) Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning (Brad Neuberg, Dropbox Engineering, 2017) Scaling Machine Learning at Uber with Michelangelo (Jeremy Hermann and Mike Del Balso, Uber Engineering, 2019) Spotify\u2019s Discover Weekly: How machine learning finds your new music (Sophia Ciocca, 2017) High Feasibility The three primary cost drivers of ML projects in order of importance are data availability, accuracy requirement, and problem difficulty. Data Availability Here are the questions you need to ask concerning the data availability: How hard is it to acquire data? How expensive is data labeling? How much data will be needed? How stable is the data? What are the data security requirements? Accuracy Requirement Here are the questions you need to ask concerning the accuracy requirement: How costly are wrong predictions? How frequently does the system need to be right to be useful? What are the ethical implications? It is worth noting that ML project costs tend to scale super-linearly in the accuracy requirement. The fundamental reason is that you typically need a lot more data and more high-quality labels to achieve high accuracy numbers. Problem Difficulty Here are the questions you need to ask concerning the problem difficulty: Is the problem well-defined? Is there good published work on similar problems? What are the computing requirements? Can a human do it? So what\u2019s still hard in machine learning? As a caveat, it\u2019s historically very challenging to predict what types of problems will be difficult for ML to solve in the future. But generally speaking, both unsupervised learning and reinforcement learning are still hard, even though they show promise in limited domains where tons of data and compute are available. Zooming into supervised learning , here are three types of hard problems: Output is complex: These are problems where the output is high-dimensional or ambiguous. Examples include 3D reconstruction, video prediction, dialog systems, open-ended recommendation systems, etc. Reliability is required: These are problems where high precision and robustness are required. Examples include systems that can fail safely in out-of-distribution scenarios, is robust to adversarial attacks, or needs to tackle highly precise tasks. Generalization is required: These are problems with out-of-distribution data or in the domains of reasoning, planning, and causality. Examples include any systems for self-driving vehicles or any systems that deal with small data. Finally, this is a nice checklist for you to run an ML feasibility assessment: Are you sure that you need ML at all? Put in the work upfront to define success criteria with all of the stakeholders. Consider the ethics of using ML. Do a literature review. Try to build a labeled benchmark dataset rapidly. Build a minimal viable product with manual rules Are you \u201creally sure\u201d that you need ML at all? 4 - Archetypes So far, we\u2019ve talked about the lifecycle and the impact of all machine learning projects. Ultimately, we generally want these projects, or applications of machine learning, to be useful for products. As we consider how ML can be applied in products, it\u2019s helpful to note that there are common machine learning product archetypes or recurrent patterns through which machine learning is applied to products. You can think of these as \u201cmental models\u201d you can use to assess your project and easily prioritize the needed resources. There are three common archetypes in machine learning projects: Software 2.0 , Human-in-the-loop , and autonomous systems . They are shown in the table below, along with common examples and questions. We\u2019ll dive deeper into each. Archetype Examples Questions Software 2.0 - Improve code completion in IDE - Build customized recommendation system - Build a better video game AI - Do your models truly improve performance? - Does performance improvement generate business value? - Do performance improvements lead to a data flywheel? Human-in-the-loop - Turn sketches into slides - Email auto-completion - Help radiologists do job faster - How good does the system need to be to be useful? - How can you collect enough data to make it good? Autonomous Systems - Full self-driving - Automated customer support - Automated website design - What is an acceptable failure rate for the system? - How can you guarantee that it won\u2019t exceed the failure rate? - How inexpensively can you label data from the system? Software 2.0 Software 2.0, which we previously alluded to from the Karpathy article , is defined as \u201c augmenting existing rules-based or deterministic software with machine learning, a probabilistic approach .\u201d Examples of this are taking a code completer in an IDE and improving the experience for the user by adding an ML component. Rather than suggesting a command based solely on the leading characters the programmer has written, you might add a model that suggests commands based on previous commands the programmer has written. As you build a software 2.0 project, strongly consider the concept of the data flywheel . For certain ML projects, as you improve your model, your product will get better and more users will engage with the product, thereby generating more data for the model to get even better. It\u2019s a classic virtuous cycle and truly the gold standard for ML projects. In embarking on creating a data flywheel, critically consider where the model could fail in relation to your product. For example, do more users lead to collecting more data that is useful for improving your model? An actual system needs to be set up to capture this data and ensure that it's meaningful for the ML lifecycle. Furthermore, consider whether more data will lead to a better model (your job as an ML practitioner) or whether a better model and better predictions will actually lead to making the product better. Ideally, you should have a quantitative assessment of what makes your product \u201cbetter\u201d and map model improvement to it. Human-in-the-Loop (HIL) HIL systems are defined as machine learning systems where the output of your model will be reviewed by a human before being executed in the real world . For example, consider translating sketches into slides. An ML algorithm can take a sketch\u2019s input and suggest to a user a particular slide design. Every output of the ML model is considered and executed upon by a human, who ultimately has to decide on the slide\u2019s design. Autonomous Systems Autonomous systems are defined as machine learning systems where the system itself makes decisions or engages in outputs that are almost never reviewed by a human . Canonically, consider the self-driving car! Feasibility Let\u2019s discuss how the product archetypes relate back to project priority. In terms of feasibility and impact, the two axes on which we consider priority, software 2.0 tends to have high feasibility but potentially low impact. The existing system is often being optimized rather than wholly replaced. However, this status with respect to priority is not static by any means. Building a data flywheel into your software 2.0 project can improve your product\u2019s impact by improving the model\u2019s performance on the task and future ones. In the case of human-in-the-loop systems, their feasibility and impact sit squarely in between autonomous systems and software 2.0. HIL systems, in particular, can benefit disproportionately in their feasibility and impact from effective product design, which naturally takes into account how humans interact with technology and can mitigate risks for machine learning model behavior. Consider how the Facebook photo tagging algorithm is implemented. Rather than tagging the user itself, the algorithm frequently asks the user to tag themselves. This effective product design allows the model to perform more effectively in the user\u2019s eye and reduces the impact of false classifications. Grammarly similarly solicits user input as part of its product design through offering explanations. Finally, recommender systems also implement this idea. In general, good product design can smooth the rough edges of ML (check out the concept of designing collaborative AI ). There are industry-leading resources that can help you merge product design and ML. Apple\u2019s ML product design guidelines suggest three key questions to anyone seeking to put ML into a product: What role does ML play in your product? How can you learn from your users? How should your app handle mistakes? Associated with each question is a set of design paradigms that help address the answers to each question. There are similarly great resources from Microsoft and Spotify . Finally, autonomous systems can see their priority improved by improving their feasibility. Specifically, you can add humans in the loop or reduce the system\u2019s natural autonomy to improve its feasibility. In the case of self-driving cars, many companies add safety drivers as guardrails to improve autonomous systems. In Voyage \u2019s case, they take a more dramatic approach of constraining the problem for the autonomous system: they only run self-driving cars in senior living communities, a narrow subset of the broader self-driving problem. 5 - Metrics So far, we\u2019ve talked about the overall ideas around picking projects and structuring them based on their archetypes and the specific considerations that go into them. Now, we\u2019ll shift gears and be a little more tactical to focus on metrics and baselines, which will help you execute projects more effectively. Choosing a Metric Metrics help us evaluate models . There\u2019s a delicate balance between the real world (which is always messy and multifaceted) and the machine learning paradigm (which optimizes a single metric) in choosing a metric. In practical production settings, we often care about multiple dimensions of performance (i.e., accuracy, speed, cost, etc.). The challenge is to reconcile all the possible evaluation methods with the reality that ML systems work best at optimizing a single number. How can we balance these competing needs in building an ML product? As you start evaluating models, choose a single metric to focus on first , such as precision, accuracy, recall, etc. This can serve as an effective first filter of performance. Subsequently, you can put together a formula that combines all the metrics you care about. Note that it\u2019s important to be flexible and regularly update this formula as your models or the requirements for the product change. Combining Metrics Two simple ways of combining metrics into a formula are averaging and thresholding . Averaging is less common but easy and intuitive; you can just take a simple average or a weighted average of the model\u2019s metrics and pick the highest average. More practically, you can apply a threshold evaluation to the model\u2019s metrics. In this method, out of n evaluation metrics, you threshold n-1 and optimize the nth metric. For example, if we look at a model\u2019s precision, memory requirement, and cost to train, we might threshold the memory requirement (no more than X MB) and the cost (no more than $X) and optimize precision (as high as possible). As you choose which metrics to threshold and what to set their threshold values to, make sure to consider domain-specific needs and the actual values of the metrics (how good/bad they might be). 6 - Baselines In any product development process, setting expectations properly is vital. For machine learning products, baselines help us set expectations for how well our model will perform . In particular, baselines set a useful lower bound for our model\u2019s performance. What\u2019s the minimum expectation we should have for a model\u2019s performance? The better defined and clear the baseline is, the more useful it is for setting the right expectations. Examples of baselines are human performance on a similar task, state-of-the-art models, or even simple heuristics. Baselines are especially important for helping decide the next steps. Consider the example below of two models with the same loss curve but differing performance with respect to the baseline. Clearly, they require different action items! As seen below, on the left, where we are starting to approach or exceed the baseline, we need to be mindful of overfitting and perhaps incorporate regularization of some sort. On the right, where the baseline hugely exceeds our model\u2019s performance, we clearly have a lot of work to do to improve the model and address its underfitting. There are a number of sources to help us define useful baselines. Broadly speaking, there are external baselines (baselines defined by others) or internal baselines you can define yourself. With internal baselines, in particular, you don\u2019t need anything too complicated, or even something with ML! Simple tests like averaging across your dataset can help you understand if your model is achieving meaningful performance. If your model can\u2019t exceed a simple baseline like this, you might need to really re-evaluate the model. Human baselines are a particularly powerful form of baseline since we often seek to replace or augment human actions. In creating these baselines, note that there\u2019s usually an inverse relationship between the quality of the baseline and the ease of data collection. In a nutshell, the harder it is to get a human baseline, the better and more useful it probably is . For example, a Mechanical Turk-created baseline is easy to generate nowadays, but the quality might be hit or miss because of the variance in annotators. However, trained, specialized annotators can be hard to acquire, but the specificity of their knowledge translates into a great baseline. Choosing where to situate your baseline on this range, from low quality/easy to high quality/hard, depends on the domain. Concentrating data collection strategically, ideally in classes where the model is least performant, is a simple way of improving the quality of the baseline. TLDR Machine learning projects are iterative. Deploy something fast to begin the cycle. Choose projects with high impact and low cost of wrong predictions. The secret sauce to make projects work well is to build automated data flywheels. In the real world, you care about many things, but you should always have just one to work on. Good baselines help you invest your effort the right way. Further Resources Andrew Ng\u2019s \u201c Machine Learning Yearning \u201d Andrej Kaparthy\u2019s \u201c Software 2.0 \u201d Agrawal, Gans, and Goldfarb\u2019s \u201c The Economics of AI \u201d Chip Huyen\u2019s \u201c Introduction to Machine Learning Systems Design \u201d Apple\u2019s \u201c Human-Interface Guidelines for Machine Learning \u201d Google\u2019s \u201c Rules of Machine Learning \u201d","title":"Lecture 5: ML Projects"},{"location":"spring2021/lecture-5/#lecture-5-ml-projects","text":"Learn how to set up Machine Learning projects like a pro. This includes an understanding of the ML lifecycle, an acute mind of the feasibility and impact, an awareness of the project archetypes, and an obsession with metrics and baselines.","title":"Lecture 5: ML Projects"},{"location":"spring2021/lecture-5/#video","text":"","title":"Video"},{"location":"spring2021/lecture-5/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-5/#detailed-notes","text":"By James Le and Vishnu Rachakonda","title":"Detailed Notes"},{"location":"spring2021/lecture-5/#1-why-do-ml-projects-fail","text":"Based on a report from TechRepublic a few years back, despite increased interest in adopting machine learning (ML) in the enterprise, 85% of machine learning projects ultimately fail to deliver on their intended promises to business. Failure can happen for many reasons; however, a few glaring dangers will cause any AI project to crash and burn. ML is still very much a research endeavor. Therefore it is very challenging to aim for a 100% success rate. Many ML projects are technically infeasible or poorly scoped. Many ML projects never leap production, thus getting stuck at the prototype phase. Many ML projects have unclear success criteria because of a lack of understanding of the value proposition. Many ML projects are poorly managed because of a lack of interest from leadership.","title":"1 - Why Do ML Projects Fail?"},{"location":"spring2021/lecture-5/#2-lifecycle","text":"It\u2019s essential to understand what constitutes all of the activities in a machine learning project. Typically speaking, there are four major phases: Planning and Project Setup : At this phase, we want to decide the problem to work on, determine the requirements and goals, figure out how to allocate resources properly, consider the ethical implications, etc. Data Collection and Labeling : At this phase, we want to collect training data and potentially annotate them with ground truth, depending on the specific sources where they come from. We may find that it\u2019s too hard to get the data, or it might be easier to label for a different task. If that\u2019s the case, go back to phase 1. Model Training and Model Debugging : At this phase, we want to implement baseline models quickly, find and reproduce state-of-the-art methods for the problem domain, debug our implementation, and improve the model performance for specific tasks. We may realize that we need to collect more data or that labeling is unreliable (thus, go back to phase 2). Or we may recognize that the task is too challenging and there is a tradeoff between project requirements (thus, go back to phase 1). Model Deploying and Model Testing : At this phase, we want to pilot the model in a constrained environment (i.e., in the lab), write tests to prevent regressions, and roll the model into production. We may see that the model doesn\u2019t work well in the lab, so we want to keep improving the model\u2019s accuracy (thus, go back to phase 3). Or we may want to fix the mismatch between training data and production data by collecting more data and mining hard cases (thus go back to phase 2). Or we may find out that the metric picked doesn\u2019t actually drive downstream user behavior, and/or performance in the real world isn\u2019t great. In such situations, we want to revisit the projects\u2019 metrics and requirements (thus, go back to phase 1). Besides the per-project activities mentioned above, there are two other things that any ML team will need to solve across any projects they get involved with: (1) building the team and hiring people; and (2) setting up infrastructure and tooling to build ML systems repeatedly and at scale. Additionally, it might be useful to understand state-of-the-art results in your application domain so that you know what\u2019s possible and what to try next.","title":"2 - Lifecycle"},{"location":"spring2021/lecture-5/#3-prioritizing-projects","text":"To prioritize projects to work on, you want to find high-impact problems and assess the potential costs associated with them. The picture below shows a general framework that encourages us to target projects with high impact and high feasibility.","title":"3 - Prioritizing Projects"},{"location":"spring2021/lecture-5/#high-impact","text":"There are no silver bullets to find high-impact ML problems to work on, but here are a few useful mental models: Where can you take advantage of cheap prediction? Where is there friction in your product? Where can you automate complicated manual processes? What are other people doing?","title":"High Impact"},{"location":"spring2021/lecture-5/#cheap-prediction","text":"In the book \u201c Prediction Machines ,\u201d the authors (Ajay Agrawal, Joshua Gans, and Avi Goldfarb) come up with an excellent mental model on the economics of Artificial Intelligence: As AI reduces the cost of prediction and prediction is central for decision making, cheap predictions would be universal for problems across business domains . Therefore, you should look for projects where cheap predictions will have a huge business impact.","title":"Cheap Prediction"},{"location":"spring2021/lecture-5/#product-needs","text":"Another lens is to think about what your product needs. In the article \u201c Three Principles for Designing ML-Powered Products ,\u201d the Spotify Design team emphasizes the importance of building ML from a product perspective and looking for parts of the product experience with high friction . Automating those parts is exactly where there is a lot of impact for ML to make your business better.","title":"Product Needs"},{"location":"spring2021/lecture-5/#ml-strength","text":"In his popular blog post \u201c Software 2.0 ,\u201d Andrej Karpathy contrasts software 1.0 (which are traditional programs with explicit instructions) and software 2.0 (where humans specify goals, while the algorithm searches for a program that works). Software 2.0 programmers work with datasets, which get compiled via optimization\u200a\u2014\u200awhich works better, more general, and less computationally expensive. Therefore, you should look for complicated rule-based software where we can learn the rules instead of programming them.","title":"ML Strength"},{"location":"spring2021/lecture-5/#inspiration-from-others","text":"Instead of reinventing the wheel, you can look at what other companies are doing. In particular, check out papers from large frontier organizations (Google, Facebook, Nvidia, Netflix, etc.) and blog posts from top earlier-stage companies (Uber, Lyft, Spotify, Stripe, etc.). Here is a list of excellent ML use cases to check out (credit to Chip Huyen\u2019s ML Systems Design Lecture 2 Note ): Human-Centric Machine Learning Infrastructure at Netflix (Ville Tuulos, InfoQ 2019) 2020 state of enterprise machine learning (Algorithmia, 2020) Using Machine Learning to Predict Value of Homes On Airbnb (Robert Chang, Airbnb Engineering & Data Science, 2017) Using Machine Learning to Improve Streaming Quality at Netflix (Chaitanya Ekanadham, Netflix Technology Blog, 2018) 150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com (Bernardi et al., KDD, 2019) How we grew from 0 to 4 million women on our fashion app, with a vertical machine learning approach (Gabriel Aldamiz, HackerNoon, 2018) Machine Learning-Powered Search Ranking of Airbnb Experiences (Mihajlo Grbovic, Airbnb Engineering & Data Science, 2019) From shallow to deep learning in fraud (Hao Yi Ong, Lyft Engineering, 2018) Space, Time and Groceries (Jeremy Stanley, Tech at Instacart, 2017) Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning (Brad Neuberg, Dropbox Engineering, 2017) Scaling Machine Learning at Uber with Michelangelo (Jeremy Hermann and Mike Del Balso, Uber Engineering, 2019) Spotify\u2019s Discover Weekly: How machine learning finds your new music (Sophia Ciocca, 2017)","title":"Inspiration From Others"},{"location":"spring2021/lecture-5/#high-feasibility","text":"The three primary cost drivers of ML projects in order of importance are data availability, accuracy requirement, and problem difficulty.","title":"High Feasibility"},{"location":"spring2021/lecture-5/#data-availability","text":"Here are the questions you need to ask concerning the data availability: How hard is it to acquire data? How expensive is data labeling? How much data will be needed? How stable is the data? What are the data security requirements?","title":"Data Availability"},{"location":"spring2021/lecture-5/#accuracy-requirement","text":"Here are the questions you need to ask concerning the accuracy requirement: How costly are wrong predictions? How frequently does the system need to be right to be useful? What are the ethical implications? It is worth noting that ML project costs tend to scale super-linearly in the accuracy requirement. The fundamental reason is that you typically need a lot more data and more high-quality labels to achieve high accuracy numbers.","title":"Accuracy Requirement"},{"location":"spring2021/lecture-5/#problem-difficulty","text":"Here are the questions you need to ask concerning the problem difficulty: Is the problem well-defined? Is there good published work on similar problems? What are the computing requirements? Can a human do it? So what\u2019s still hard in machine learning? As a caveat, it\u2019s historically very challenging to predict what types of problems will be difficult for ML to solve in the future. But generally speaking, both unsupervised learning and reinforcement learning are still hard, even though they show promise in limited domains where tons of data and compute are available. Zooming into supervised learning , here are three types of hard problems: Output is complex: These are problems where the output is high-dimensional or ambiguous. Examples include 3D reconstruction, video prediction, dialog systems, open-ended recommendation systems, etc. Reliability is required: These are problems where high precision and robustness are required. Examples include systems that can fail safely in out-of-distribution scenarios, is robust to adversarial attacks, or needs to tackle highly precise tasks. Generalization is required: These are problems with out-of-distribution data or in the domains of reasoning, planning, and causality. Examples include any systems for self-driving vehicles or any systems that deal with small data. Finally, this is a nice checklist for you to run an ML feasibility assessment: Are you sure that you need ML at all? Put in the work upfront to define success criteria with all of the stakeholders. Consider the ethics of using ML. Do a literature review. Try to build a labeled benchmark dataset rapidly. Build a minimal viable product with manual rules Are you \u201creally sure\u201d that you need ML at all?","title":"Problem Difficulty"},{"location":"spring2021/lecture-5/#4-archetypes","text":"So far, we\u2019ve talked about the lifecycle and the impact of all machine learning projects. Ultimately, we generally want these projects, or applications of machine learning, to be useful for products. As we consider how ML can be applied in products, it\u2019s helpful to note that there are common machine learning product archetypes or recurrent patterns through which machine learning is applied to products. You can think of these as \u201cmental models\u201d you can use to assess your project and easily prioritize the needed resources. There are three common archetypes in machine learning projects: Software 2.0 , Human-in-the-loop , and autonomous systems . They are shown in the table below, along with common examples and questions. We\u2019ll dive deeper into each. Archetype Examples Questions Software 2.0 - Improve code completion in IDE - Build customized recommendation system - Build a better video game AI - Do your models truly improve performance? - Does performance improvement generate business value? - Do performance improvements lead to a data flywheel? Human-in-the-loop - Turn sketches into slides - Email auto-completion - Help radiologists do job faster - How good does the system need to be to be useful? - How can you collect enough data to make it good? Autonomous Systems - Full self-driving - Automated customer support - Automated website design - What is an acceptable failure rate for the system? - How can you guarantee that it won\u2019t exceed the failure rate? - How inexpensively can you label data from the system?","title":"4 - Archetypes"},{"location":"spring2021/lecture-5/#software-20","text":"Software 2.0, which we previously alluded to from the Karpathy article , is defined as \u201c augmenting existing rules-based or deterministic software with machine learning, a probabilistic approach .\u201d Examples of this are taking a code completer in an IDE and improving the experience for the user by adding an ML component. Rather than suggesting a command based solely on the leading characters the programmer has written, you might add a model that suggests commands based on previous commands the programmer has written. As you build a software 2.0 project, strongly consider the concept of the data flywheel . For certain ML projects, as you improve your model, your product will get better and more users will engage with the product, thereby generating more data for the model to get even better. It\u2019s a classic virtuous cycle and truly the gold standard for ML projects. In embarking on creating a data flywheel, critically consider where the model could fail in relation to your product. For example, do more users lead to collecting more data that is useful for improving your model? An actual system needs to be set up to capture this data and ensure that it's meaningful for the ML lifecycle. Furthermore, consider whether more data will lead to a better model (your job as an ML practitioner) or whether a better model and better predictions will actually lead to making the product better. Ideally, you should have a quantitative assessment of what makes your product \u201cbetter\u201d and map model improvement to it.","title":"Software 2.0"},{"location":"spring2021/lecture-5/#human-in-the-loop-hil","text":"HIL systems are defined as machine learning systems where the output of your model will be reviewed by a human before being executed in the real world . For example, consider translating sketches into slides. An ML algorithm can take a sketch\u2019s input and suggest to a user a particular slide design. Every output of the ML model is considered and executed upon by a human, who ultimately has to decide on the slide\u2019s design.","title":"Human-in-the-Loop (HIL)"},{"location":"spring2021/lecture-5/#autonomous-systems","text":"Autonomous systems are defined as machine learning systems where the system itself makes decisions or engages in outputs that are almost never reviewed by a human . Canonically, consider the self-driving car!","title":"Autonomous Systems"},{"location":"spring2021/lecture-5/#feasibility","text":"Let\u2019s discuss how the product archetypes relate back to project priority. In terms of feasibility and impact, the two axes on which we consider priority, software 2.0 tends to have high feasibility but potentially low impact. The existing system is often being optimized rather than wholly replaced. However, this status with respect to priority is not static by any means. Building a data flywheel into your software 2.0 project can improve your product\u2019s impact by improving the model\u2019s performance on the task and future ones. In the case of human-in-the-loop systems, their feasibility and impact sit squarely in between autonomous systems and software 2.0. HIL systems, in particular, can benefit disproportionately in their feasibility and impact from effective product design, which naturally takes into account how humans interact with technology and can mitigate risks for machine learning model behavior. Consider how the Facebook photo tagging algorithm is implemented. Rather than tagging the user itself, the algorithm frequently asks the user to tag themselves. This effective product design allows the model to perform more effectively in the user\u2019s eye and reduces the impact of false classifications. Grammarly similarly solicits user input as part of its product design through offering explanations. Finally, recommender systems also implement this idea. In general, good product design can smooth the rough edges of ML (check out the concept of designing collaborative AI ). There are industry-leading resources that can help you merge product design and ML. Apple\u2019s ML product design guidelines suggest three key questions to anyone seeking to put ML into a product: What role does ML play in your product? How can you learn from your users? How should your app handle mistakes? Associated with each question is a set of design paradigms that help address the answers to each question. There are similarly great resources from Microsoft and Spotify . Finally, autonomous systems can see their priority improved by improving their feasibility. Specifically, you can add humans in the loop or reduce the system\u2019s natural autonomy to improve its feasibility. In the case of self-driving cars, many companies add safety drivers as guardrails to improve autonomous systems. In Voyage \u2019s case, they take a more dramatic approach of constraining the problem for the autonomous system: they only run self-driving cars in senior living communities, a narrow subset of the broader self-driving problem.","title":"Feasibility"},{"location":"spring2021/lecture-5/#5-metrics","text":"So far, we\u2019ve talked about the overall ideas around picking projects and structuring them based on their archetypes and the specific considerations that go into them. Now, we\u2019ll shift gears and be a little more tactical to focus on metrics and baselines, which will help you execute projects more effectively.","title":"5 - Metrics"},{"location":"spring2021/lecture-5/#choosing-a-metric","text":"Metrics help us evaluate models . There\u2019s a delicate balance between the real world (which is always messy and multifaceted) and the machine learning paradigm (which optimizes a single metric) in choosing a metric. In practical production settings, we often care about multiple dimensions of performance (i.e., accuracy, speed, cost, etc.). The challenge is to reconcile all the possible evaluation methods with the reality that ML systems work best at optimizing a single number. How can we balance these competing needs in building an ML product? As you start evaluating models, choose a single metric to focus on first , such as precision, accuracy, recall, etc. This can serve as an effective first filter of performance. Subsequently, you can put together a formula that combines all the metrics you care about. Note that it\u2019s important to be flexible and regularly update this formula as your models or the requirements for the product change.","title":"Choosing a Metric"},{"location":"spring2021/lecture-5/#combining-metrics","text":"Two simple ways of combining metrics into a formula are averaging and thresholding . Averaging is less common but easy and intuitive; you can just take a simple average or a weighted average of the model\u2019s metrics and pick the highest average. More practically, you can apply a threshold evaluation to the model\u2019s metrics. In this method, out of n evaluation metrics, you threshold n-1 and optimize the nth metric. For example, if we look at a model\u2019s precision, memory requirement, and cost to train, we might threshold the memory requirement (no more than X MB) and the cost (no more than $X) and optimize precision (as high as possible). As you choose which metrics to threshold and what to set their threshold values to, make sure to consider domain-specific needs and the actual values of the metrics (how good/bad they might be).","title":"Combining Metrics"},{"location":"spring2021/lecture-5/#6-baselines","text":"In any product development process, setting expectations properly is vital. For machine learning products, baselines help us set expectations for how well our model will perform . In particular, baselines set a useful lower bound for our model\u2019s performance. What\u2019s the minimum expectation we should have for a model\u2019s performance? The better defined and clear the baseline is, the more useful it is for setting the right expectations. Examples of baselines are human performance on a similar task, state-of-the-art models, or even simple heuristics. Baselines are especially important for helping decide the next steps. Consider the example below of two models with the same loss curve but differing performance with respect to the baseline. Clearly, they require different action items! As seen below, on the left, where we are starting to approach or exceed the baseline, we need to be mindful of overfitting and perhaps incorporate regularization of some sort. On the right, where the baseline hugely exceeds our model\u2019s performance, we clearly have a lot of work to do to improve the model and address its underfitting. There are a number of sources to help us define useful baselines. Broadly speaking, there are external baselines (baselines defined by others) or internal baselines you can define yourself. With internal baselines, in particular, you don\u2019t need anything too complicated, or even something with ML! Simple tests like averaging across your dataset can help you understand if your model is achieving meaningful performance. If your model can\u2019t exceed a simple baseline like this, you might need to really re-evaluate the model. Human baselines are a particularly powerful form of baseline since we often seek to replace or augment human actions. In creating these baselines, note that there\u2019s usually an inverse relationship between the quality of the baseline and the ease of data collection. In a nutshell, the harder it is to get a human baseline, the better and more useful it probably is . For example, a Mechanical Turk-created baseline is easy to generate nowadays, but the quality might be hit or miss because of the variance in annotators. However, trained, specialized annotators can be hard to acquire, but the specificity of their knowledge translates into a great baseline. Choosing where to situate your baseline on this range, from low quality/easy to high quality/hard, depends on the domain. Concentrating data collection strategically, ideally in classes where the model is least performant, is a simple way of improving the quality of the baseline.","title":"6 - Baselines"},{"location":"spring2021/lecture-5/#tldr","text":"Machine learning projects are iterative. Deploy something fast to begin the cycle. Choose projects with high impact and low cost of wrong predictions. The secret sauce to make projects work well is to build automated data flywheels. In the real world, you care about many things, but you should always have just one to work on. Good baselines help you invest your effort the right way.","title":"TLDR"},{"location":"spring2021/lecture-5/#further-resources","text":"Andrew Ng\u2019s \u201c Machine Learning Yearning \u201d Andrej Kaparthy\u2019s \u201c Software 2.0 \u201d Agrawal, Gans, and Goldfarb\u2019s \u201c The Economics of AI \u201d Chip Huyen\u2019s \u201c Introduction to Machine Learning Systems Design \u201d Apple\u2019s \u201c Human-Interface Guidelines for Machine Learning \u201d Google\u2019s \u201c Rules of Machine Learning \u201d","title":"Further Resources"},{"location":"spring2021/lecture-6/","text":"Lecture 6: Infrastructure & Tooling Video Slides PDF Download Detailed Notes 1 - Dream vs. Reality for ML Practitioners The dream of ML practitioners is that we are provided the data, and somehow we build an optimal machine learning prediction system available as a scalable API or an edge deployment. That deployment then generates more data for us, which can be used to improve our system. The reality is that you will have to: Aggregate, process, clean, label, and version the data Write and debug model code Provision compute Run many experiments and review the results Discover that you did something wrong or maybe try a different architecture -> Write more code and provision more compute Deploy the model when you are happy Monitor the predictions that the model makes on production data so that you can gather some good examples and feed them back to the initial data flywheel loop For example, the slide above is from Andrej Karpathy\u2019s talk at PyTorch Devcon 2019 discussing Tesla\u2019s self-driving system. Their dream is to build a system that goes from the data gathered through their training, evaluation, and inference processes and gets deployed on the cars. As people drive, more data will be collected and added back to the training set. As this process repeats, Tesla\u2019s ML engineers can all go on vacation :) The picture above (from the famous Google paper \u201c Machine Learning: The High-Interest Credit Card of Technical Debt \u201d) shows that the ML code portion in a real-world ML system is a lot smaller than the infrastructure needed for its support . As ML projects move from small-scale research experiments to large-scale industry deployments, your organization most likely will require a massive amount of infrastructure to support large inferences, distributed training, data processing pipelines, reproducible experiments, model monitoring, etc. 2 - Three Buckets of Tooling Landscape We can break down the landscape of all this necessary infrastructure into three buckets: data, training/evaluation, and deployment. The data bucket includes the data sources, data lakes/warehouses, data processing, data exploration, data versioning, and data labeling. The training/evaluation bucket includes compute sources, resource management, software engineering, frameworks and distributed training libraries, experiment management, and hyper-parameter tuning. The deployment bucket includes continuous integration and testing, edge deployment, web deployment, monitoring, and feature store. There are also several vendors offering \u201call-in-one\u201d MLOps solutions that cover all three buckets. This lecture focuses on the training/evaluation bucket. 3 - Software Engineering When it comes to writing deep learning code, Python is the clear programming language of choice . As a general-purpose language, Python is easy to learn and easily accessible, enabling you to find skilled developers on a faster basis. It has various scientific libraries for data wrangling and machine learning (Pandas, NumPy, Scikit-Learn, etc.). Regardless of whether your engineering colleagues write code in a lower-level language like C, C++, or Java, it is generally neat to join different components with a Python wrapper. When choosing your IDEs, there are many options out there (Vim, Emacs, Sublime Text, Jupyter, VS Code, PyCharm, Atom, etc.). Each of these has its uses in any application, and you\u2019re better to switch between them to remain agile without relying heavily on shortcuts and packages. It also helps teams work better if they can jump into different IDEs and comment/collaborate with other colleagues. In particular, Visual Studio Code makes for a very nice Python experience, where you have access to built-in git staging and diffing, peek at documentation, linter code as you write, and open projects remotely. Jupyter Notebooks have rapidly grown in popularity among data scientists to become the standard for quick prototyping and exploratory analysis. For example, Netflix based all of their machine learning workflows on them , effectively building a whole notebook infrastructure to leverage them as a unifying layer for scheduling workflows. Jeremy Howard develops his fast.ai codebase entirely with notebooks and introduces a project called nbdev that shows people how to develop well-tested code in a notebook environment. However, there are many problems with using notebooks as a last resort when working in teams that aim to build machine/deep learning products . Alexander Mueller's blog post outlines the five reasons why they suck: It is challenging to enable good code versioning because notebooks are big JSON files that cannot be merged automatically. Notebook \u201cIDE\u201d is primitive , as they have no integration, no lifting, and no code-style correction. Data scientists are not software engineers, and thus, tools that govern their code quality and help improve it are very important. It is very hard to structure code reasonably, put code into functions, and develop tests while working in notebooks. You better develop Python scripts based on test-driven development principles as soon as you want to reproduce some experiments and run notebooks frequently. Notebooks have out-of-order execution artifacts , meaning that you can easily destroy your current working state when jumping between cells of notebooks. It is also difficult to run long or distributed tasks . If you want to handle big datasets, better pull your code out of notebooks, start a Python folder, create fixtures, write tests, and then deploy your application to a cluster. Recently, a new application framework called Streamlit was introduced. The creators of the framework wanted machine learning engineers to be able to create beautiful apps without needing a tools team; in other words, these internal tools should arise as a natural byproduct of the machine learning workflow. According to the launch blog post , here are the core principles of Streamlit: Embrace Python scripting : Streamlit apps are just scripts that run from top to bottom. There\u2019s no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. Treat widgets as variables : There are no callbacks in Streamlit. Every interaction simply reruns the script from top to bottom. This approach leads to a clean codebase. Reuse data and computation : Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default data store that lets Streamlit apps safely and effortlessly reuse information. Right now, Streamlit is building features that enable sharing machine learning projects to be as easy as pushing a web app to Heroku. We recommend using conda to set up your Python and CUDA environments and pip-tools to separate mutually compatible versions of all requirements for our lab . 4 - Compute Hardware We can break down the compute needs into an early-stage development step and a late-stage training/evaluation step. During the development stage, we write code, debug models, and look at the results. It\u2019d be nice to be able to compile and train models via an intuitive GUI quickly. During the training/evaluation stage, we design model architecture, search for hyper-parameters, and train large models. It\u2019d be nice to launch experiments and review results easily. Compute matters with each passing year due to the fact that the results came out of deep learning are using more and more compute (check out this 2018 report from OpenAI ). Looking at recent Transformer models, while OpenAI\u2019s GPT-3 has not been fully commercialized yet, Google already released the Switch Transformer with orders of magnitude larger in the number of parameters. So should you get your own hardware, go straight to the cloud, or use on-premise options? GPU Basics This is basically an NVIDIA game, as they are the only provider of good deep learning GPUs. However, Google\u2019s TPUs are the fastest, which is available only on GCP. There is a new NVIDIA architecture every year: Kepler -> Pascal -> Volta -> Turing -> Ampere. NVIDIA often released the server version of the cards first, then the \u201centhusiast\u201d version, and finally the consumer version. If you use these cards for business purposes, then you suppose to use the server version. GPUs have a different amount of RAM. You can only compute on the data that is on the GPU memory. The more data you can fit on the GPU, the larger your batches are, the faster your training goes . For deep learning, you use 32-bit precision. In fact, starting with the Volta architecture, NVIDIA developed tensor cores that are specifically designed for deep learning operations (mixed-precision between 32 and 16 bit). Tensor Cores reduce the used cycles needed for calculating multiply and addition operations and the reliance on repetitive shared memory access, thus saving additional cycles for memory access. This is very useful for the convolutional/Transformer models that are prevalent nowadays. Let\u2019s go through different GPU architectures: Kepler / Maxwell : They are 2-4x slower than the Pascal/Volta ones below. You should not buy these old guards (K80). Pascal : They are in the 1080 Ti cards from 2017, which are still useful if bought used (especially for recurrent neural networks). P100 is the equivalent cloud offering. Volta / Turing : These are the preferred choices over the Kepler and Pascal because of their support for 16-bit mixed-precision via tensor cores. Hardware options are 2080 Ti and Titan RTX, while the cloud option is V100. Ampere : This architecture is available in the latest hardware (3090) and cloud (A100) offerings. They have the most tensor cores, leading to at least 30% speedup over Turing. You can check out this recent GPU benchmark from Lambda Labs and consult Tim Dettmers\u2019 advice on which GPUs to get . Cloud Options Amazon Web Services, Google Cloud Platform, and Microsoft Azure are the cloud heavyweights with largely similar functions and prices. There are also startups like Lambda Labs and Corewave that provide cloud GPUs. On-Prem Options You can either build your own or buy pre-built devices from vendors like Lambda Labs, NVIDIA, Supermicro, Cirrascale, etc. Recommendations Even though the cloud is expensive, it\u2019s hard to make on-prem devices scale past a certain point. Furthermore, dev-ops things are easier to be done in the cloud than to be set up by yourself. And if your machine dies or requires maintenance, that will be a constant headache if you are responsible for managing it. Here are our recommendations for three profiles: Hobbyists: Build your own machine (maybe a 4x Turing or a 2x Ampere PC) during development. Either use the same PC or use cloud instances during training/evaluation. Startups: Buy a sizeable Lambda Labs machine for every ML scientist during development. Buy more shared server machines or use cloud instances during training/evaluation. Larger companies: Buy an even more powerful machine for every ML scientist during development. Use cloud with fast instances with proper provisioning and handling of failures during training/evaluation. 5 - Resource Management With all the resources we have discussed (compute, dependencies, etc.), our challenge turns to manage them across the specific use cases we may have. Across all the resources, our goal is always to be able to easily experiment with the necessary resources to achieve the desired application of ML for our product. For this challenge of allocating resources to experimenting users, there are some common solutions: Script a solution ourselves : In theory, this is the simplest solution. We can check if a resource is free and then lock it if a particular user is using it or wants to. SLURM : If we don't want to write the script entirely ourselves, standard cluster job schedulers like SLURM can help us. The workflow is as follows: First, a script defines a job\u2019s requirements. Then, the SLURM queue runner analyzes this and then executes the jobs on the correct resource. Docker/Kubernetes : The above approach might still be too manual for your needs, in which case you can turn to Docker/Kubernetes. Docker packages the dependency stack into a lighter-than-VM package called a container (that excludes the OS). Kubernetes lets us run these Docker containers on a cluster. In particular, Kubeflow is an OSS project started by Google that allows you to spawn/manage Jupyter notebooks and manage multi-step workflows. It also has lots of plug-ins for extra processes like hyperparameter tuning and model deployment. However, Kubeflow can be a challenge to setup. Custom ML software : There\u2019s a lot of novel work and all-in-one solutions being developed to provision compute resources for ML development efficiently. Platforms like AWS Sagemaker , Paperspace Gradient , and Determined AI are advancing. Newer startups like Anyscale and Grid.AI (creators of PyTorch Lightning) are also tackling this. Their vision is around allowing you to seamlessly go from training models on your computer to running lots of training jobs in the cloud with a simple set of SDK commands. 6 - Frameworks and Distributed Training Deep Learning Frameworks If you\u2019ve built a deep learning model in the last few years, you\u2019ve probably used a deep learning framework. Frameworks like TensorFlow have crucially shaped the development of the deep learning revolution. The reality is that deep learning frameworks have existed for a while. Projects like Theano and Torch have been around for 10+ years. In contemporary use, there are three main frameworks we\u2019ll focus on - TensorFlow , Keras , and PyTorch . We evaluate frameworks based on their utility for production and development . When TensorFlow came out in 2015, it was billed heavily as a production-optimized DL framework with an underlying static optimized graph that could be deployed across compute environments. However, TF 1.0 had a pretty unpleasant development experience; in addition to developing your models, you had to consider the underlying execution graph you were describing. This kind of \u201cmeta-development\u201d posed a challenge for newcomers. The Keras project solved many of these issues by offering a simpler way to define models, and eventually became a part of TensorFlow. PyTorch, when it was introduced in 2017, offered a polar opposite to TensorFlow. It made development super easy by consisting almost exclusively of simple Python commands, but was not designed to be fast at scale. Using TF/Keras or PyTorch is the current recommended way to build deep learning models unless you have a powerful reason not to. Essentially, both have converged to pretty similar points that balance development and production. TensorFlow adopted eager execution by default and became a lot easier to develop quickly in. PyTorch subsumed Caffe2 and became much faster as a result, specifically by adding the ability to compile speedier model artifacts. Nowadays, PyTorch has a lot of momentum, likely due to its ease of development. Newer projects like fast.ai and PyTorch Lighting add best practices and additional functionality to PyTorch, making it even more popular. According to this 2018 article on The Gradient , more than 80% of submissions are in PyTorch in academic projects. All these frameworks may seem like excessive quibbling, especially since PyTorch and TensorFlow have converged in important ways. Why do we even require such extensive frameworks? It\u2019s theoretically possible to define entire models and their required matrix math (e.g., a CNN) in NumPy, the classic Python numerical computing library. However, we quickly run into two challenges: back-propagating errors through our model and running the code on GPUs, which are powerful computation accelerators. For these issues to be addressed, we need frameworks to help us with auto-differentiation , an efficient way of computing the gradients, and software compatibility with GPUs , specifically interfacing with CUDA. Frameworks allow us to abstract the work required to achieve both features, while also layering in valuable abstractions for all the latest layer designs, optimizers, losses, and much more. As you can imagine, the abstractions offered by frameworks save us valuable time on getting our model to run and allow us to focus on optimizing our model. New projects like JAX and HuggingFace offer different or simpler abstractions. JAX focuses primarily on fast numerical computation with autodiff and GPUs across machine learning use cases (not just deep learning). HuggingFace abstracts entire model architectures in the NLP realm. Instead of loading individual layers, HuggingFace lets you load the entirety of a contemporary mode (along with weights)l like BERT, tremendously speeding up development time. HuggingFace works on both PyTorch and TensorFlow. Distributed Training Distributed training is a hot topic as the datasets and the models we train become too large to work on a single GPU. It\u2019s increasingly a must-do. The important thing to note is that distributed training is a process to conduct a single model training process ; don\u2019t confuse it with training multiple models on different GPUs. There are two approaches to distributed training: data parallelism and model parallelism. Data Parallelism Data parallelism is quite simple but powerful. If we have a batch size of X samples, which is too large for one GPU, we can split the X samples evenly across N GPUs. Each GPU calculates the gradients and passes them to a central node (either a GPU or a CPU), where the gradients are averaged and backpropagated through the distributed GPUs. This paradigm generally results in a linear speed-up time (e.g., two distributed GPUs results in a ~2X speed-up in training time). In modern frameworks like PyTorch, PyTorch Lightning, and even in schedulers like SLURM, data-parallel training can be achieved simply by specifying the number of GPUs or calling a data parallelism-enabling object (e.g., torch.nn.DataParallel ). Other tools like Horovod (from Uber) use non-framework-specific ways of enabling data parallelism (e.g., MPI, a standard multiprocessing framework). Ray , the original open-source project from the Anyscale team, was designed to enable general distributed computing applications in Python and can be similarly applied to data-parallel distributed training. Model Parallelism Model parallelism is a lot more complicated. If you can\u2019t fit your entire model\u2019s weights on a single GPU, you can split the weights across GPUs and pass data through each to train the weights. This usually adds a lot of complexity and should be avoided unless absolutely necessary. A better solution is to pony up for the best GPU available, either locally or in the cloud. You can also use gradient checkpointing, a clever trick wherein you write some gradients to disk as you compute them and load them only as you need them for updates. New work is coming out to make this easier (e.g., research and framework maturity). 7 - Experiment Management As you run numerous experiments to refine your model, it\u2019s easy to lose track of code, hyperparameters, and artifacts. Model iteration can lead to lots of complexity and messiness. For example, you could be monitoring the learning rate\u2019s impact on your model\u2019s performance metric. With multiple model runs, how will you monitor the impact of the hyperparameter? A low-tech way would be to manually track the results of all model runs in a spreadsheet. Without great attention to detail, this can quickly spiral into a messy or incomplete artifact. Dedicated experiment management platforms are a remedy to this issue. Let\u2019s cover a few of the most common ones: TensorBoard : This is the default experiment tracking platform that comes with TensorFlow. As a pro, it\u2019s easy to get started with. On the flip side, it\u2019s not very good for tracking and comparing multiple experiments. It\u2019s also not the best solution to store past work. MLFlow : An OSS project from Databricks, MLFlow is a complete platform for the ML lifecycle. They have great experiment and model run management at the core of their platform. Another open-source project, Keepsake , recently came out focused solely on experiment tracking. Paid platforms ( Comet.ml , Weights and Biases , Neptune ): Finally, outside vendors offer deep, thorough experiment management platforms, with tools like code diffs, report writing, data visualization, and model registering features. In our labs, we will use Weights and Biases. 8 - Hyperparameter Tuning To finalize models, we need to ensure that we have the optimal hyperparameters. Since hyperparameter optimization (as this process is called) can be a particularly compute-intensive process, it\u2019s useful to have software that can help. Using specific software can help us kill underperforming model runs with bad hyperparameters early (to save on cost) or help us intelligently sweep ranges of hyperparameter values. Luckily, there\u2019s an increasing number of software providers that do precisely this: SigOpt offers an API focused exclusively on efficient, iterative hyperparameter optimization. Specify a range of values, get SigOpt\u2019s recommended hyperparameter settings, run the model and return the results to SigOpt, and repeat the process until you\u2019ve found the best parameters for your model. Rather than an API, Ray Tune offers a local software (part of the broader Ray ecosystem) that integrates hyperparameter optimization with compute resource allocation. Jobs are scheduled with specific hyperparameters according to state-of-the-art methods, and underperforming jobs are automatically killed. Weights and Biases also has this feature! With a YAML file specification, we can specify a hyperparameter optimization job and perform a \u201c sweep ,\u201d during which W&B sends parameter settings to individual \u201cagents\u201d (our machines) and compares performance. 9 - \u201cAll-In-One\u201d Solutions Some platforms integrate all the aspects of the applied ML stack we\u2019ve discussed (experiment tracking, optimization, training, etc.) and wrap them into a single experience. To support the \u201clifecycle,\u201d these platforms typically include: Labeling and data querying services Model training, especially though job scaling and scheduling Experiment tracking and model versioning Development environments, typically through notebook-style interfaces Model deployment (e.g., via REST APIs) and monitoring One of the earliest examples of such a system is Facebook\u2019s FBLearner (2016), which encompassed data and feature storage, training, inference, and continuous learning based on user interactions with the model\u2019s outputs. You can imagine how powerful having one hub for all this activity can be for ML application and development speed. As a result, cloud vendors (Google, AWS, Azure) have developed similar all-in-one platforms, like Google Cloud AI Platform and AWS SageMaker . Startups like Paperspace Gradient , Neptune , and FloydHub also offer all-in-one platforms focused on deep learning. Determined AI , which focuses exclusively on the model development and training part of the lifecycle, is the rare open-source platform in this space. Domino Data Lab is a traditional ML-focused startup with an extensive feature set worth looking at. It\u2019s natural to expect more MLOps (as this kind of tooling and infra is referred to) companies and vendors to build out their feature set and become platform-oriented; Weights and Biases is a good example of this. In conclusion, take a look at the below table to compare a select number of MLOps platform vendors. Pricing is quite variable. Staying up to date across all the tooling can be a real challenge, but check out FSDL\u2019s Tooling Tuesdays on Twitter as a starting point!","title":"Lecture 6: Infrastructure & Tooling"},{"location":"spring2021/lecture-6/#lecture-6-infrastructure-tooling","text":"","title":"Lecture 6: Infrastructure &amp; Tooling"},{"location":"spring2021/lecture-6/#video","text":"","title":"Video"},{"location":"spring2021/lecture-6/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-6/#detailed-notes","text":"","title":"Detailed Notes"},{"location":"spring2021/lecture-6/#1-dream-vs-reality-for-ml-practitioners","text":"The dream of ML practitioners is that we are provided the data, and somehow we build an optimal machine learning prediction system available as a scalable API or an edge deployment. That deployment then generates more data for us, which can be used to improve our system. The reality is that you will have to: Aggregate, process, clean, label, and version the data Write and debug model code Provision compute Run many experiments and review the results Discover that you did something wrong or maybe try a different architecture -> Write more code and provision more compute Deploy the model when you are happy Monitor the predictions that the model makes on production data so that you can gather some good examples and feed them back to the initial data flywheel loop For example, the slide above is from Andrej Karpathy\u2019s talk at PyTorch Devcon 2019 discussing Tesla\u2019s self-driving system. Their dream is to build a system that goes from the data gathered through their training, evaluation, and inference processes and gets deployed on the cars. As people drive, more data will be collected and added back to the training set. As this process repeats, Tesla\u2019s ML engineers can all go on vacation :) The picture above (from the famous Google paper \u201c Machine Learning: The High-Interest Credit Card of Technical Debt \u201d) shows that the ML code portion in a real-world ML system is a lot smaller than the infrastructure needed for its support . As ML projects move from small-scale research experiments to large-scale industry deployments, your organization most likely will require a massive amount of infrastructure to support large inferences, distributed training, data processing pipelines, reproducible experiments, model monitoring, etc.","title":"1 - Dream vs. Reality for ML Practitioners"},{"location":"spring2021/lecture-6/#2-three-buckets-of-tooling-landscape","text":"We can break down the landscape of all this necessary infrastructure into three buckets: data, training/evaluation, and deployment. The data bucket includes the data sources, data lakes/warehouses, data processing, data exploration, data versioning, and data labeling. The training/evaluation bucket includes compute sources, resource management, software engineering, frameworks and distributed training libraries, experiment management, and hyper-parameter tuning. The deployment bucket includes continuous integration and testing, edge deployment, web deployment, monitoring, and feature store. There are also several vendors offering \u201call-in-one\u201d MLOps solutions that cover all three buckets. This lecture focuses on the training/evaluation bucket.","title":"2 - Three Buckets of Tooling Landscape"},{"location":"spring2021/lecture-6/#3-software-engineering","text":"When it comes to writing deep learning code, Python is the clear programming language of choice . As a general-purpose language, Python is easy to learn and easily accessible, enabling you to find skilled developers on a faster basis. It has various scientific libraries for data wrangling and machine learning (Pandas, NumPy, Scikit-Learn, etc.). Regardless of whether your engineering colleagues write code in a lower-level language like C, C++, or Java, it is generally neat to join different components with a Python wrapper. When choosing your IDEs, there are many options out there (Vim, Emacs, Sublime Text, Jupyter, VS Code, PyCharm, Atom, etc.). Each of these has its uses in any application, and you\u2019re better to switch between them to remain agile without relying heavily on shortcuts and packages. It also helps teams work better if they can jump into different IDEs and comment/collaborate with other colleagues. In particular, Visual Studio Code makes for a very nice Python experience, where you have access to built-in git staging and diffing, peek at documentation, linter code as you write, and open projects remotely. Jupyter Notebooks have rapidly grown in popularity among data scientists to become the standard for quick prototyping and exploratory analysis. For example, Netflix based all of their machine learning workflows on them , effectively building a whole notebook infrastructure to leverage them as a unifying layer for scheduling workflows. Jeremy Howard develops his fast.ai codebase entirely with notebooks and introduces a project called nbdev that shows people how to develop well-tested code in a notebook environment. However, there are many problems with using notebooks as a last resort when working in teams that aim to build machine/deep learning products . Alexander Mueller's blog post outlines the five reasons why they suck: It is challenging to enable good code versioning because notebooks are big JSON files that cannot be merged automatically. Notebook \u201cIDE\u201d is primitive , as they have no integration, no lifting, and no code-style correction. Data scientists are not software engineers, and thus, tools that govern their code quality and help improve it are very important. It is very hard to structure code reasonably, put code into functions, and develop tests while working in notebooks. You better develop Python scripts based on test-driven development principles as soon as you want to reproduce some experiments and run notebooks frequently. Notebooks have out-of-order execution artifacts , meaning that you can easily destroy your current working state when jumping between cells of notebooks. It is also difficult to run long or distributed tasks . If you want to handle big datasets, better pull your code out of notebooks, start a Python folder, create fixtures, write tests, and then deploy your application to a cluster. Recently, a new application framework called Streamlit was introduced. The creators of the framework wanted machine learning engineers to be able to create beautiful apps without needing a tools team; in other words, these internal tools should arise as a natural byproduct of the machine learning workflow. According to the launch blog post , here are the core principles of Streamlit: Embrace Python scripting : Streamlit apps are just scripts that run from top to bottom. There\u2019s no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps. Treat widgets as variables : There are no callbacks in Streamlit. Every interaction simply reruns the script from top to bottom. This approach leads to a clean codebase. Reuse data and computation : Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default data store that lets Streamlit apps safely and effortlessly reuse information. Right now, Streamlit is building features that enable sharing machine learning projects to be as easy as pushing a web app to Heroku. We recommend using conda to set up your Python and CUDA environments and pip-tools to separate mutually compatible versions of all requirements for our lab .","title":"3 - Software Engineering"},{"location":"spring2021/lecture-6/#4-compute-hardware","text":"We can break down the compute needs into an early-stage development step and a late-stage training/evaluation step. During the development stage, we write code, debug models, and look at the results. It\u2019d be nice to be able to compile and train models via an intuitive GUI quickly. During the training/evaluation stage, we design model architecture, search for hyper-parameters, and train large models. It\u2019d be nice to launch experiments and review results easily. Compute matters with each passing year due to the fact that the results came out of deep learning are using more and more compute (check out this 2018 report from OpenAI ). Looking at recent Transformer models, while OpenAI\u2019s GPT-3 has not been fully commercialized yet, Google already released the Switch Transformer with orders of magnitude larger in the number of parameters. So should you get your own hardware, go straight to the cloud, or use on-premise options?","title":"4 - Compute Hardware"},{"location":"spring2021/lecture-6/#gpu-basics","text":"This is basically an NVIDIA game, as they are the only provider of good deep learning GPUs. However, Google\u2019s TPUs are the fastest, which is available only on GCP. There is a new NVIDIA architecture every year: Kepler -> Pascal -> Volta -> Turing -> Ampere. NVIDIA often released the server version of the cards first, then the \u201centhusiast\u201d version, and finally the consumer version. If you use these cards for business purposes, then you suppose to use the server version. GPUs have a different amount of RAM. You can only compute on the data that is on the GPU memory. The more data you can fit on the GPU, the larger your batches are, the faster your training goes . For deep learning, you use 32-bit precision. In fact, starting with the Volta architecture, NVIDIA developed tensor cores that are specifically designed for deep learning operations (mixed-precision between 32 and 16 bit). Tensor Cores reduce the used cycles needed for calculating multiply and addition operations and the reliance on repetitive shared memory access, thus saving additional cycles for memory access. This is very useful for the convolutional/Transformer models that are prevalent nowadays. Let\u2019s go through different GPU architectures: Kepler / Maxwell : They are 2-4x slower than the Pascal/Volta ones below. You should not buy these old guards (K80). Pascal : They are in the 1080 Ti cards from 2017, which are still useful if bought used (especially for recurrent neural networks). P100 is the equivalent cloud offering. Volta / Turing : These are the preferred choices over the Kepler and Pascal because of their support for 16-bit mixed-precision via tensor cores. Hardware options are 2080 Ti and Titan RTX, while the cloud option is V100. Ampere : This architecture is available in the latest hardware (3090) and cloud (A100) offerings. They have the most tensor cores, leading to at least 30% speedup over Turing. You can check out this recent GPU benchmark from Lambda Labs and consult Tim Dettmers\u2019 advice on which GPUs to get .","title":"GPU Basics"},{"location":"spring2021/lecture-6/#cloud-options","text":"Amazon Web Services, Google Cloud Platform, and Microsoft Azure are the cloud heavyweights with largely similar functions and prices. There are also startups like Lambda Labs and Corewave that provide cloud GPUs.","title":"Cloud Options"},{"location":"spring2021/lecture-6/#on-prem-options","text":"You can either build your own or buy pre-built devices from vendors like Lambda Labs, NVIDIA, Supermicro, Cirrascale, etc.","title":"On-Prem Options"},{"location":"spring2021/lecture-6/#recommendations","text":"Even though the cloud is expensive, it\u2019s hard to make on-prem devices scale past a certain point. Furthermore, dev-ops things are easier to be done in the cloud than to be set up by yourself. And if your machine dies or requires maintenance, that will be a constant headache if you are responsible for managing it. Here are our recommendations for three profiles: Hobbyists: Build your own machine (maybe a 4x Turing or a 2x Ampere PC) during development. Either use the same PC or use cloud instances during training/evaluation. Startups: Buy a sizeable Lambda Labs machine for every ML scientist during development. Buy more shared server machines or use cloud instances during training/evaluation. Larger companies: Buy an even more powerful machine for every ML scientist during development. Use cloud with fast instances with proper provisioning and handling of failures during training/evaluation.","title":"Recommendations"},{"location":"spring2021/lecture-6/#5-resource-management","text":"With all the resources we have discussed (compute, dependencies, etc.), our challenge turns to manage them across the specific use cases we may have. Across all the resources, our goal is always to be able to easily experiment with the necessary resources to achieve the desired application of ML for our product. For this challenge of allocating resources to experimenting users, there are some common solutions: Script a solution ourselves : In theory, this is the simplest solution. We can check if a resource is free and then lock it if a particular user is using it or wants to. SLURM : If we don't want to write the script entirely ourselves, standard cluster job schedulers like SLURM can help us. The workflow is as follows: First, a script defines a job\u2019s requirements. Then, the SLURM queue runner analyzes this and then executes the jobs on the correct resource. Docker/Kubernetes : The above approach might still be too manual for your needs, in which case you can turn to Docker/Kubernetes. Docker packages the dependency stack into a lighter-than-VM package called a container (that excludes the OS). Kubernetes lets us run these Docker containers on a cluster. In particular, Kubeflow is an OSS project started by Google that allows you to spawn/manage Jupyter notebooks and manage multi-step workflows. It also has lots of plug-ins for extra processes like hyperparameter tuning and model deployment. However, Kubeflow can be a challenge to setup. Custom ML software : There\u2019s a lot of novel work and all-in-one solutions being developed to provision compute resources for ML development efficiently. Platforms like AWS Sagemaker , Paperspace Gradient , and Determined AI are advancing. Newer startups like Anyscale and Grid.AI (creators of PyTorch Lightning) are also tackling this. Their vision is around allowing you to seamlessly go from training models on your computer to running lots of training jobs in the cloud with a simple set of SDK commands.","title":"5 - Resource Management"},{"location":"spring2021/lecture-6/#6-frameworks-and-distributed-training","text":"","title":"6 - Frameworks and Distributed Training"},{"location":"spring2021/lecture-6/#deep-learning-frameworks","text":"If you\u2019ve built a deep learning model in the last few years, you\u2019ve probably used a deep learning framework. Frameworks like TensorFlow have crucially shaped the development of the deep learning revolution. The reality is that deep learning frameworks have existed for a while. Projects like Theano and Torch have been around for 10+ years. In contemporary use, there are three main frameworks we\u2019ll focus on - TensorFlow , Keras , and PyTorch . We evaluate frameworks based on their utility for production and development . When TensorFlow came out in 2015, it was billed heavily as a production-optimized DL framework with an underlying static optimized graph that could be deployed across compute environments. However, TF 1.0 had a pretty unpleasant development experience; in addition to developing your models, you had to consider the underlying execution graph you were describing. This kind of \u201cmeta-development\u201d posed a challenge for newcomers. The Keras project solved many of these issues by offering a simpler way to define models, and eventually became a part of TensorFlow. PyTorch, when it was introduced in 2017, offered a polar opposite to TensorFlow. It made development super easy by consisting almost exclusively of simple Python commands, but was not designed to be fast at scale. Using TF/Keras or PyTorch is the current recommended way to build deep learning models unless you have a powerful reason not to. Essentially, both have converged to pretty similar points that balance development and production. TensorFlow adopted eager execution by default and became a lot easier to develop quickly in. PyTorch subsumed Caffe2 and became much faster as a result, specifically by adding the ability to compile speedier model artifacts. Nowadays, PyTorch has a lot of momentum, likely due to its ease of development. Newer projects like fast.ai and PyTorch Lighting add best practices and additional functionality to PyTorch, making it even more popular. According to this 2018 article on The Gradient , more than 80% of submissions are in PyTorch in academic projects. All these frameworks may seem like excessive quibbling, especially since PyTorch and TensorFlow have converged in important ways. Why do we even require such extensive frameworks? It\u2019s theoretically possible to define entire models and their required matrix math (e.g., a CNN) in NumPy, the classic Python numerical computing library. However, we quickly run into two challenges: back-propagating errors through our model and running the code on GPUs, which are powerful computation accelerators. For these issues to be addressed, we need frameworks to help us with auto-differentiation , an efficient way of computing the gradients, and software compatibility with GPUs , specifically interfacing with CUDA. Frameworks allow us to abstract the work required to achieve both features, while also layering in valuable abstractions for all the latest layer designs, optimizers, losses, and much more. As you can imagine, the abstractions offered by frameworks save us valuable time on getting our model to run and allow us to focus on optimizing our model. New projects like JAX and HuggingFace offer different or simpler abstractions. JAX focuses primarily on fast numerical computation with autodiff and GPUs across machine learning use cases (not just deep learning). HuggingFace abstracts entire model architectures in the NLP realm. Instead of loading individual layers, HuggingFace lets you load the entirety of a contemporary mode (along with weights)l like BERT, tremendously speeding up development time. HuggingFace works on both PyTorch and TensorFlow.","title":"Deep Learning Frameworks"},{"location":"spring2021/lecture-6/#distributed-training","text":"Distributed training is a hot topic as the datasets and the models we train become too large to work on a single GPU. It\u2019s increasingly a must-do. The important thing to note is that distributed training is a process to conduct a single model training process ; don\u2019t confuse it with training multiple models on different GPUs. There are two approaches to distributed training: data parallelism and model parallelism.","title":"Distributed Training"},{"location":"spring2021/lecture-6/#data-parallelism","text":"Data parallelism is quite simple but powerful. If we have a batch size of X samples, which is too large for one GPU, we can split the X samples evenly across N GPUs. Each GPU calculates the gradients and passes them to a central node (either a GPU or a CPU), where the gradients are averaged and backpropagated through the distributed GPUs. This paradigm generally results in a linear speed-up time (e.g., two distributed GPUs results in a ~2X speed-up in training time). In modern frameworks like PyTorch, PyTorch Lightning, and even in schedulers like SLURM, data-parallel training can be achieved simply by specifying the number of GPUs or calling a data parallelism-enabling object (e.g., torch.nn.DataParallel ). Other tools like Horovod (from Uber) use non-framework-specific ways of enabling data parallelism (e.g., MPI, a standard multiprocessing framework). Ray , the original open-source project from the Anyscale team, was designed to enable general distributed computing applications in Python and can be similarly applied to data-parallel distributed training.","title":"Data Parallelism"},{"location":"spring2021/lecture-6/#model-parallelism","text":"Model parallelism is a lot more complicated. If you can\u2019t fit your entire model\u2019s weights on a single GPU, you can split the weights across GPUs and pass data through each to train the weights. This usually adds a lot of complexity and should be avoided unless absolutely necessary. A better solution is to pony up for the best GPU available, either locally or in the cloud. You can also use gradient checkpointing, a clever trick wherein you write some gradients to disk as you compute them and load them only as you need them for updates. New work is coming out to make this easier (e.g., research and framework maturity).","title":"Model Parallelism"},{"location":"spring2021/lecture-6/#7-experiment-management","text":"As you run numerous experiments to refine your model, it\u2019s easy to lose track of code, hyperparameters, and artifacts. Model iteration can lead to lots of complexity and messiness. For example, you could be monitoring the learning rate\u2019s impact on your model\u2019s performance metric. With multiple model runs, how will you monitor the impact of the hyperparameter? A low-tech way would be to manually track the results of all model runs in a spreadsheet. Without great attention to detail, this can quickly spiral into a messy or incomplete artifact. Dedicated experiment management platforms are a remedy to this issue. Let\u2019s cover a few of the most common ones: TensorBoard : This is the default experiment tracking platform that comes with TensorFlow. As a pro, it\u2019s easy to get started with. On the flip side, it\u2019s not very good for tracking and comparing multiple experiments. It\u2019s also not the best solution to store past work. MLFlow : An OSS project from Databricks, MLFlow is a complete platform for the ML lifecycle. They have great experiment and model run management at the core of their platform. Another open-source project, Keepsake , recently came out focused solely on experiment tracking. Paid platforms ( Comet.ml , Weights and Biases , Neptune ): Finally, outside vendors offer deep, thorough experiment management platforms, with tools like code diffs, report writing, data visualization, and model registering features. In our labs, we will use Weights and Biases.","title":"7 - Experiment Management"},{"location":"spring2021/lecture-6/#8-hyperparameter-tuning","text":"To finalize models, we need to ensure that we have the optimal hyperparameters. Since hyperparameter optimization (as this process is called) can be a particularly compute-intensive process, it\u2019s useful to have software that can help. Using specific software can help us kill underperforming model runs with bad hyperparameters early (to save on cost) or help us intelligently sweep ranges of hyperparameter values. Luckily, there\u2019s an increasing number of software providers that do precisely this: SigOpt offers an API focused exclusively on efficient, iterative hyperparameter optimization. Specify a range of values, get SigOpt\u2019s recommended hyperparameter settings, run the model and return the results to SigOpt, and repeat the process until you\u2019ve found the best parameters for your model. Rather than an API, Ray Tune offers a local software (part of the broader Ray ecosystem) that integrates hyperparameter optimization with compute resource allocation. Jobs are scheduled with specific hyperparameters according to state-of-the-art methods, and underperforming jobs are automatically killed. Weights and Biases also has this feature! With a YAML file specification, we can specify a hyperparameter optimization job and perform a \u201c sweep ,\u201d during which W&B sends parameter settings to individual \u201cagents\u201d (our machines) and compares performance.","title":"8 - Hyperparameter Tuning"},{"location":"spring2021/lecture-6/#9-all-in-one-solutions","text":"Some platforms integrate all the aspects of the applied ML stack we\u2019ve discussed (experiment tracking, optimization, training, etc.) and wrap them into a single experience. To support the \u201clifecycle,\u201d these platforms typically include: Labeling and data querying services Model training, especially though job scaling and scheduling Experiment tracking and model versioning Development environments, typically through notebook-style interfaces Model deployment (e.g., via REST APIs) and monitoring One of the earliest examples of such a system is Facebook\u2019s FBLearner (2016), which encompassed data and feature storage, training, inference, and continuous learning based on user interactions with the model\u2019s outputs. You can imagine how powerful having one hub for all this activity can be for ML application and development speed. As a result, cloud vendors (Google, AWS, Azure) have developed similar all-in-one platforms, like Google Cloud AI Platform and AWS SageMaker . Startups like Paperspace Gradient , Neptune , and FloydHub also offer all-in-one platforms focused on deep learning. Determined AI , which focuses exclusively on the model development and training part of the lifecycle, is the rare open-source platform in this space. Domino Data Lab is a traditional ML-focused startup with an extensive feature set worth looking at. It\u2019s natural to expect more MLOps (as this kind of tooling and infra is referred to) companies and vendors to build out their feature set and become platform-oriented; Weights and Biases is a good example of this. In conclusion, take a look at the below table to compare a select number of MLOps platform vendors. Pricing is quite variable. Staying up to date across all the tooling can be a real challenge, but check out FSDL\u2019s Tooling Tuesdays on Twitter as a starting point!","title":"9 - \u201cAll-In-One\u201d Solutions"},{"location":"spring2021/lecture-7/","text":"Lecture 7: Troubleshooting Deep Neural Networks Video Slides PDF Download Detailed Notes Notes were taken by James Le and Vishnu Rachakonda In traditional software engineering, a bug usually leads to the program crashing. While this is annoying for the user, it is critical for the developer to inspect the errors to understand why. With deep learning, we sometimes encounter errors, but all too often, the program crashes without a clear reason why. While these issues can be debugged manually, deep learning models most often fail because of poor output predictions. What\u2019s worse is that when the model performance is low, there is usually no signal about why or when the models failed. A common sentiment among practitioners is that they spend 80\u201390% of time debugging and tuning the models and only 10\u201320% of time deriving math equations and implementing things. This is confirmed by Andrej Kaparthy, as seen in this tweet . 1 - Why Is Deep Learning Troubleshooting Hard? Suppose you are trying to reproduce a research paper result for your work, but your results are worse. You might wonder why your model\u2019s performance is significantly worse than the paper that you\u2019re trying to reproduce? Many different things can cause this: It can be implementation bugs . Most bugs in deep learning are actually invisible. Hyper-parameter choices can also cause your performance to degrade. Deep learning models are very sensitive to hyper-parameters. Even very subtle choices of learning rate and weight initialization can make a big difference. Performance can also be worse just because of data/model fit . For example, you pre-train your model on ImageNet data and fit it on self-driving car images, which are harder to learn. Finally, poor model performance could be caused not by your model but your dataset construction . Typical issues here include not having enough examples, dealing with noisy labels and imbalanced classes, splitting train and test set with different distributions. 2 - Strategy to Debug Neural Networks The key idea of deep learning troubleshooting is: Since it is hard to disambiguate errors, it\u2019s best to start simple and gradually ramp up complexity. This lecture provides a decision tree for debugging deep learning models and improving performance . This guide assumes that you already have an initial test dataset, a single metric to improve, and target performance based on human-level performance, published results, previous baselines, etc. 3 - Start Simple The first step is the troubleshooting workflow is starting simple . Choose A Simple Architecture There are a few things to consider when you want to start simple. The first is how to choose a simple architecture . These are architectures that are easy to implement and are likely to get you part of the way towards solving your problem without introducing as many bugs. Architecture selection is one of the many intimidating parts of getting into deep learning because there are tons of papers coming out all-the-time and claiming to be state-of-the-art on some problems. They get very complicated fast. In the limit, if you\u2019re trying to get to maximal performance, then architecture selection is challenging. But when starting on a new problem, you can just solve a simple set of rules that will allow you to pick an architecture that enables you to do a decent job on the problem you\u2019re working on. If your data looks like images , start with a LeNet-like architecture and consider using something like ResNet as your codebase gets more mature. If your data looks like sequences , start with an LSTM with one hidden layer and/or temporal/classical convolutions. Then, when your problem gets more mature, you can move to an Attention-based model or a WaveNet-like model. For all other tasks , start with a fully-connected neural network with one hidden layer and use more advanced networks later, depending on the problem. In reality, many times, the input data contains multiple of those things above. So how to deal with multiple input modalities into a neural network? Here is the 3-step strategy that we recommend: First, map each of these modalities into a lower-dimensional feature space. In the example above, the images are passed through a ConvNet, and the words are passed through an LSTM. Then we flatten the outputs of those networks to get a single vector for each of the inputs that will go into the model. Then we concatenate those inputs. Finally, we pass them through some fully-connected layers to an output. Use Sensible Defaults After choosing a simple architecture, the next thing to do is to select sensible hyper-parameter defaults to start with. Here are the defaults that we recommend: Adam optimizer with a \u201cmagic\u201d learning rate value of 3e-4 . ReLU activation for fully-connected and convolutional models and Tanh activation for LSTM models. He initialization for ReLU activation function and Glorot initialization for Tanh activation function . No regularization and data normalization. Normalize Inputs The next step is to normalize the input data , subtracting the mean and dividing by the variance. Note that for images, it\u2019s fine to scale values to [0, 1] or [-0.5, 0.5] (for example, by dividing by 255). Simplify The Problem The final thing you should do is consider simplifying the problem itself. If you have a complicated problem with massive data and tons of classes to deal with, then you should consider: Working with a small training set around 10,000 examples. Using a fixed number of objects, classes, input size, etc. Creating a simpler synthetic training set like in research labs. This is important because (1) you will have reasonable confidence that your model should be able to solve, and (2) your iteration speed will increase. The diagram below neatly summarizes how to start simple: 4 - Implement and Debug To give you a preview, below are the five most common bugs in deep learning models that we recognize: Incorrect shapes for the network tensors : This bug is a common one and can fail silently. This happens many times because the automatic differentiation systems in the deep learning framework do silent broadcasting. Tensors become different shapes in the network and can cause a lot of problems. Pre-processing inputs incorrectly : For example, you forget to normalize your inputs or apply too much input pre-processing (over-normalization and excessive data augmentation). Incorrect input to the model\u2019s loss function : For example, you use softmax outputs to a loss that expects logits. Forgot to set up train mode for the network correctly : For example, toggling train/evaluation mode or controlling batch norm dependencies. Numerical instability : For example, you get `inf` or `NaN` as outputs. This bug often stems from using an exponent, a log, or a division operation somewhere in the code. Here are three pieces of general advice for implementing your model: Start with a lightweight implementation . You want minimum possible new lines of code for the 1st version of your model. The rule of thumb is less than 200 lines. This doesn\u2019t count tested infrastructure components or TensorFlow/PyTorch code. Use off-the-shelf components such as Keras if possible, since most of the stuff in Keras works well out-of-the-box. If you have to use TensorFlow, use the built-in functions, don\u2019t do the math yourself. This would help you avoid a lot of numerical instability issues. Build complicated data pipelines later . These are important for large-scale ML systems, but you should not start with them because data pipelines themselves can be a big source of bugs. Just start with a dataset that you can load into memory. Get Your Model To Run The first step of implementing bug-free deep learning models is getting your model to run at all . There are a few things that can prevent this from happening: Shape mismatch/casting issue : To address this type of problem, you should step through your model creation and inference step-by-step in a debugger, checking for correct shapes and data types of your tensors. Out-of-memory issues : This can be very difficult to debug. You can scale back your memory-intensive operations one-by-one. For example, if you create large matrices anywhere in your code, you can reduce the size of their dimensions or cut your batch size in half. Other issues : You can simply Google it. Stack Overflow would be great most of the time. Let\u2019s zoom in on the process of stepping through model creation in a debugger and talk about debuggers for deep learning code : In PyTorch, you can use ipdb \u200a\u2014\u200awhich exports functions to access the interactive IPython debugger. In TensorFlow, it\u2019s trickier. TensorFlow separates the process of creating the graph and executing operations in the graph. There are three options you can try: (1) step through the graph creation itself and inspect each tensor layer, (2) step into the training loop and evaluate the tensor layers, or (3) use TensorFlow Debugger (tfdb), which does option 1 and 2 automatically. Overfit A Single Batch After getting your model to run, the next thing you need to do is to overfit a single batch of data . This is a heuristic that can catch an absurd number of bugs. This really means that you want to drive your training error arbitrarily close to 0. There are a few things that can happen when you try to overfit a single batch and it fails: Error goes up : Commonly, this is due to a flip sign somewhere in the loss function/gradient. Error explodes : This is usually a numerical issue but can also be caused by a high learning rate. Error oscillates : You can lower the learning rate and inspect the data for shuffled labels or incorrect data augmentation. Error plateaus : You can increase the learning rate and get rid of regulation. Then you can inspect the loss function and the data pipeline for correctness. Compare To A Known Result Once your model overfits in a single batch, there can still be some other issues that cause bugs. The last step here is to compare your results to a known result . So what sort of known results are useful? The most useful results come from an official model implementation evaluated on a similar dataset to yours . You can step through the code in both models line-by-line and ensure your model has the same output. You want to ensure that your model performance is up to par with expectations. If you can\u2019t find an official implementation on a similar dataset, you can compare your approach to results from an official model implementation evaluated on a benchmark dataset . You most definitely want to walk through the code line-by-line and ensure you have the same output. If there is no official implementation of your approach, you can compare it to results from an unofficial model implementation . You can review the code the same as before but with lower confidence (because almost all the unofficial implementations on GitHub have bugs). Then, you can compare to results from a paper with no code (to ensure that your performance is up to par with expectations), results from your model on a benchmark dataset (to make sure your model performs well in a simpler setting), and results from a similar model on a similar dataset (to help you get a general sense of what kind of performance can be expected). An under-rated source of results comes from simple baselines (for example, the average of outputs or linear regression), which can help make sure that your model is learning anything at all. The diagram below neatly summarizes how to implement and debug deep neural networks: 5 - Evaluate Bias-Variance Decomposition To evaluate models and prioritize the next steps in model development, we will apply the bias-variance decomposition. The bias-variance decomposition is the fundamental model fitting tradeoff. In our application, let\u2019s talk more specifically about the formula for bias-variance tradeoff with respect to the test error; this will help us apply the concept more directly to our model\u2019s performance. There are four terms in the formula for test error: Test error = irreducible error + bias + variance + validation overfitting Irreducible error is the baseline error you don\u2019t expect your model to do better. It can be estimated through strong baselines, like human performance. Avoidable bias , a measure of underfitting, is the difference between our train error and irreducible error. Variance , a measure of overfitting, is the difference between validation error and training error. Validation set overfitting is the difference between test error and validation error. Consider the chart of learning curves and errors below. Using the test error formula for bias and variance, we can calculate each component of test error and make decisions based on the value. For example, our avoidable bias is rather low (only 2 points), while the variance is much higher (5 points). With this knowledge, we should prioritize methods of preventing overfitting, like regularization. Distribution Shift Clearly, the application of the bias-variance decomposition to the test error has already helped prioritize our next steps for model development. However, until now, we\u2019ve assumed that the samples (training, validation, testing) all come from the same distribution. What if this isn\u2019t the case? In practical ML situations, this distribution shift often cars. In building self-driving cars, a frequent occurrence might be training with samples from one distribution (e.g., daytime driving video) but testing or inferring on samples from a totally different distribution (e.g., night time driving). A simple way of handling this wrinkle in our assumption is to create two validation sets: one from the training distribution and one from the test distribution. This can be helpful even with a very small testing set. If we apply this, we can actually estimate our distribution shift, which is the difference between testing validation error and testing error. This is really useful for practical applications of ML! With this new term, let\u2019s update our test error formula of bias and variance: Test error = irreducible error + bias + variance + distribution shift + validation overfitting 6 - Improve Model and Data Using the updated formula from the last section, we\u2019ll be able to decide on and prioritize the right next steps for each iteration of a model. In particular, we\u2019ll follow a specific process (shown below). Step 1: Address Underfitting We\u2019ll start by addressing underfitting (i.e., reducing bias). The first thing to try in this case is to make your model bigger (e.g., add layers, more units per layer). Next, consider regularization, which can prevent a tight fit to your data. Other options are error analysis, choosing a different model architecture (e.g., something more state of the art), tuning hyperparameters, or adding features. Some notes: Choosing different architectures, especially a SOTA one, can be very helpful but is also risky. Bugs are easily introduced in the implementation process. Adding features is uncommon in the deep learning paradigm (vs. traditional machine learning). We usually want the network to learn features of its own accord. If all else fails, it can be beneficial in a practical setting. Step 2: Address Overfitting After addressing underfitting, move on to solving overfitting. Similarly, there\u2019s a recommended series of methods to try in order. Starting with collecting training data (if possible) is the soundest way to address overfitting, though it can be challenging in certain applications. Next, tactical improvements like normalization, data augmentation, and regularization can help. Following these steps, traditional defaults like tuning hyperparameters, choosing a different architecture, or error analysis are useful. Finally, if overfitting is rather intractable, there\u2019s a series of less recommended steps, such as early stopping, removing features, and reducing model size. Early stopping is a personal choice; the fast.ai community is a strong proponent. Step 3: Address Distribution Shift After addressing underfitting and overfitting, If there\u2019s a difference between the error on our training validation set vs. our test validation set, we need to address the error caused by the distribution shift. This is a harder problem to solve, so there\u2019s less in our toolkit to apply. Start by looking manually at the errors in the test-validation set. Compare the potential logic behind these errors to the performance in the train-validation set, and use the errors to guide further data collection. Essentially, reason about why your model may be suffering from distribution shift error. This is the most principled way to deal with distribution shift, though it\u2019s the most challenging way practically. If collecting more data to address these errors isn\u2019t possible, try synthesizing data. Additionally, you can try domain adaptation . Error Analysis Manually evaluating errors to understand model performance is generally a high-yield way of figuring out how to improve the model. Systematically performing this error analysis process and decomposing the error from different error types can help prioritize model improvements. For example, in a self-driving car use case with error types like hard-to-see pedestrians, reflections, and nighttime scenes, decomposing the error contribution of each and where it occurs (train-val vs. test-val) can give rise to a clear set of prioritized action items. See the table for an example of how this error analysis can be effectively structured. Domain Adaptation Domain adaptation is a class of techniques that train on a \u201csource\u201d distribution and generalize to another \u201ctarget\u201d using only unlabeled data or limited labeled data. You should use domain adaptation when access to labeled data from the test distribution is limited, but access to relatively similar data is plentiful. There are a few different types of domain adaptation: Supervised domain adaptation : In this case, we have limited data from the target domain to adapt to. Some example applications of the concept include fine-tuning a pre-trained model or adding target data to a training set. Unsupervised domain adaptation : In this case, we have lots of unlabeled data from the target domain. Some techniques you might see are CORAL, domain confusion, and CycleGAN. Practically speaking, supervised domain adaptation can work really well! Unsupervised domain adaptation has a little bit further to go. Step 4: Rebalance datasets If the test-validation set performance starts to look considerably better than the test performance, you may have overfit the validation set. This commonly occurs with small validation sets or lots of hyperparameter training. If this occurs, resample the validation set from the test distribution and get a fresh estimate of the performance. 7 - Tune Hyperparameters One of the core challenges in hyperparameter optimization is very basic: which hyperparameters should you tune? As we consider this fundamental question, let\u2019s keep the following in mind: Models are more sensitive to some hyperparameters than others. This means we should focus our efforts on the more impactful hyperparameters. However, which hyperparameters are most important depends heavily on our choice of model. Certain rules of thumbs can help guide our initial thinking. Sensitivity is always relative to default values; if you use good defaults, you might start in a good place! See the following table for a ranked list of hyperparameters and their impact on the model: Techniques for Tuning Hyperparameter Optimization Now that we know which hyperparameters make the most sense to tune (using rules of thumb), let\u2019s consider the various methods of actually tuning them: Manual Hyperparameter Optimization . Colloquially referred to as Graduate Student Descent, this method works by taking a manual, detailed look at your algorithm, building intuition, and considering which hyperparameters would make the most difference. After figuring out these parameters, you train, evaluate, and guess a better hyperparameter value using your intuition for the algorithm and intelligence. While it may seem archaic, this method combines well with other methods (e.g., setting a range of values for hyperparameters) and has the main benefit of reducing computation time and cost if used skillfully. It can be time-consuming and challenging, but it can be a good starting point. Grid Search . Imagine each of your parameters plotted against each other on a grid, from which you uniformly sample values to test. For each point, you run a training run and evaluate performance. The advantages are that it\u2019s very simple and can often produce good results. However, it\u2019s quite inefficient, as you must run every combination of hyperparameters. It also often requires prior knowledge about the hyperparameters since we must manually set the range of values. Random Search : This method is recommended over grid search. Rather than sampling from the grid of values for the hyperparameter evenly, we\u2019ll choose n points sampled randomly across the grid. Empirically, this method produces better results than grid search. However, the results can be somewhat uninterpretable, with unexpected values in certain hyperparameters returned. Coarse-to-fine Search : Rather than running entirely random runs, we can gradually narrow in on the best hyperparameters through this method. Initially, start by defining a very large range to run a randomized search on. Within the pool of results, you can find N best results and hone in on the hyperparameter values used to generate those samples. As you iteratively perform this method, you can get excellent performance. This doesn\u2019t remove the manual component, as you have to select which range to continuously narrow your search to, but it\u2019s perhaps the most popular method available. Bayesian Hyperparameter Optimization : This is a reasonably sophisticated method, which you can read more about here and here . At a high level, start with a prior estimate of parameter distributions. Subsequently, maintain a probabilistic model of the relationship between hyperparameter values and model performance. As you maintain this model, you toggle between training with hyperparameter values that maximize the expected improvement (per the model) and use training results to update the initial probabilistic model and its expectations. This is a great, hands-off, efficient method to choose hyperparameters. However, these techniques can be quite challenging to implement from scratch. As libraries and infrastructure mature, the integration of these methods into training will become easier. In summary, you should probably start with coarse-to-fine random searches and move to Bayesian methods as your codebase matures and you\u2019re more certain of your model. 8 - Conclusion To wrap up this lecture, deep learning troubleshooting and debugging is really hard. It\u2019s difficult to tell if you have a bug because there are many possible sources for the same degradation in performance. Furthermore, the results can be sensitive to small changes in hyper-parameters and dataset makeup. To train bug-free deep learning models, we need to treat building them as an iterative process. If you skipped to the end, the following steps can make this process easier and catch errors as early as possible: Start Simple : Choose the simplest model and data possible. Implement and Debug : Once the model runs, overfit a single batch and reproduce a known result. Evaluate : Apply the bias-variance decomposition to decide what to do next. Tune Hyper-parameters : Use coarse-to-fine random searches to tune the model\u2019s hyper-parameters. Improve Model and Data : Make your model bigger if your model under-fits and add more data and/or regularization if your model over-fits. Here are additional resources that you can go to learn more: Andrew Ng\u2019s \u201c Machine Learning Yearning \u201d book. This Twitter thread from Andrej Karpathy. BYU\u2019s \u201c Practical Advice for Building Deep Neural Networks \u201d blog post.","title":"Lecture 7: Troubleshooting Deep Neural Networks"},{"location":"spring2021/lecture-7/#lecture-7-troubleshooting-deep-neural-networks","text":"","title":"Lecture 7: Troubleshooting Deep Neural Networks"},{"location":"spring2021/lecture-7/#video","text":"","title":"Video"},{"location":"spring2021/lecture-7/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-7/#detailed-notes","text":"Notes were taken by James Le and Vishnu Rachakonda In traditional software engineering, a bug usually leads to the program crashing. While this is annoying for the user, it is critical for the developer to inspect the errors to understand why. With deep learning, we sometimes encounter errors, but all too often, the program crashes without a clear reason why. While these issues can be debugged manually, deep learning models most often fail because of poor output predictions. What\u2019s worse is that when the model performance is low, there is usually no signal about why or when the models failed. A common sentiment among practitioners is that they spend 80\u201390% of time debugging and tuning the models and only 10\u201320% of time deriving math equations and implementing things. This is confirmed by Andrej Kaparthy, as seen in this tweet .","title":"Detailed Notes"},{"location":"spring2021/lecture-7/#1-why-is-deep-learning-troubleshooting-hard","text":"Suppose you are trying to reproduce a research paper result for your work, but your results are worse. You might wonder why your model\u2019s performance is significantly worse than the paper that you\u2019re trying to reproduce? Many different things can cause this: It can be implementation bugs . Most bugs in deep learning are actually invisible. Hyper-parameter choices can also cause your performance to degrade. Deep learning models are very sensitive to hyper-parameters. Even very subtle choices of learning rate and weight initialization can make a big difference. Performance can also be worse just because of data/model fit . For example, you pre-train your model on ImageNet data and fit it on self-driving car images, which are harder to learn. Finally, poor model performance could be caused not by your model but your dataset construction . Typical issues here include not having enough examples, dealing with noisy labels and imbalanced classes, splitting train and test set with different distributions.","title":"1 - Why Is Deep Learning Troubleshooting Hard?"},{"location":"spring2021/lecture-7/#2-strategy-to-debug-neural-networks","text":"The key idea of deep learning troubleshooting is: Since it is hard to disambiguate errors, it\u2019s best to start simple and gradually ramp up complexity. This lecture provides a decision tree for debugging deep learning models and improving performance . This guide assumes that you already have an initial test dataset, a single metric to improve, and target performance based on human-level performance, published results, previous baselines, etc.","title":"2 - Strategy to Debug Neural Networks"},{"location":"spring2021/lecture-7/#3-start-simple","text":"The first step is the troubleshooting workflow is starting simple .","title":"3 - Start Simple"},{"location":"spring2021/lecture-7/#choose-a-simple-architecture","text":"There are a few things to consider when you want to start simple. The first is how to choose a simple architecture . These are architectures that are easy to implement and are likely to get you part of the way towards solving your problem without introducing as many bugs. Architecture selection is one of the many intimidating parts of getting into deep learning because there are tons of papers coming out all-the-time and claiming to be state-of-the-art on some problems. They get very complicated fast. In the limit, if you\u2019re trying to get to maximal performance, then architecture selection is challenging. But when starting on a new problem, you can just solve a simple set of rules that will allow you to pick an architecture that enables you to do a decent job on the problem you\u2019re working on. If your data looks like images , start with a LeNet-like architecture and consider using something like ResNet as your codebase gets more mature. If your data looks like sequences , start with an LSTM with one hidden layer and/or temporal/classical convolutions. Then, when your problem gets more mature, you can move to an Attention-based model or a WaveNet-like model. For all other tasks , start with a fully-connected neural network with one hidden layer and use more advanced networks later, depending on the problem. In reality, many times, the input data contains multiple of those things above. So how to deal with multiple input modalities into a neural network? Here is the 3-step strategy that we recommend: First, map each of these modalities into a lower-dimensional feature space. In the example above, the images are passed through a ConvNet, and the words are passed through an LSTM. Then we flatten the outputs of those networks to get a single vector for each of the inputs that will go into the model. Then we concatenate those inputs. Finally, we pass them through some fully-connected layers to an output.","title":"Choose A Simple Architecture"},{"location":"spring2021/lecture-7/#use-sensible-defaults","text":"After choosing a simple architecture, the next thing to do is to select sensible hyper-parameter defaults to start with. Here are the defaults that we recommend: Adam optimizer with a \u201cmagic\u201d learning rate value of 3e-4 . ReLU activation for fully-connected and convolutional models and Tanh activation for LSTM models. He initialization for ReLU activation function and Glorot initialization for Tanh activation function . No regularization and data normalization.","title":"Use Sensible Defaults"},{"location":"spring2021/lecture-7/#normalize-inputs","text":"The next step is to normalize the input data , subtracting the mean and dividing by the variance. Note that for images, it\u2019s fine to scale values to [0, 1] or [-0.5, 0.5] (for example, by dividing by 255).","title":"Normalize Inputs"},{"location":"spring2021/lecture-7/#simplify-the-problem","text":"The final thing you should do is consider simplifying the problem itself. If you have a complicated problem with massive data and tons of classes to deal with, then you should consider: Working with a small training set around 10,000 examples. Using a fixed number of objects, classes, input size, etc. Creating a simpler synthetic training set like in research labs. This is important because (1) you will have reasonable confidence that your model should be able to solve, and (2) your iteration speed will increase. The diagram below neatly summarizes how to start simple:","title":"Simplify The Problem"},{"location":"spring2021/lecture-7/#4-implement-and-debug","text":"To give you a preview, below are the five most common bugs in deep learning models that we recognize: Incorrect shapes for the network tensors : This bug is a common one and can fail silently. This happens many times because the automatic differentiation systems in the deep learning framework do silent broadcasting. Tensors become different shapes in the network and can cause a lot of problems. Pre-processing inputs incorrectly : For example, you forget to normalize your inputs or apply too much input pre-processing (over-normalization and excessive data augmentation). Incorrect input to the model\u2019s loss function : For example, you use softmax outputs to a loss that expects logits. Forgot to set up train mode for the network correctly : For example, toggling train/evaluation mode or controlling batch norm dependencies. Numerical instability : For example, you get `inf` or `NaN` as outputs. This bug often stems from using an exponent, a log, or a division operation somewhere in the code. Here are three pieces of general advice for implementing your model: Start with a lightweight implementation . You want minimum possible new lines of code for the 1st version of your model. The rule of thumb is less than 200 lines. This doesn\u2019t count tested infrastructure components or TensorFlow/PyTorch code. Use off-the-shelf components such as Keras if possible, since most of the stuff in Keras works well out-of-the-box. If you have to use TensorFlow, use the built-in functions, don\u2019t do the math yourself. This would help you avoid a lot of numerical instability issues. Build complicated data pipelines later . These are important for large-scale ML systems, but you should not start with them because data pipelines themselves can be a big source of bugs. Just start with a dataset that you can load into memory.","title":"4 - Implement and Debug"},{"location":"spring2021/lecture-7/#get-your-model-to-run","text":"The first step of implementing bug-free deep learning models is getting your model to run at all . There are a few things that can prevent this from happening: Shape mismatch/casting issue : To address this type of problem, you should step through your model creation and inference step-by-step in a debugger, checking for correct shapes and data types of your tensors. Out-of-memory issues : This can be very difficult to debug. You can scale back your memory-intensive operations one-by-one. For example, if you create large matrices anywhere in your code, you can reduce the size of their dimensions or cut your batch size in half. Other issues : You can simply Google it. Stack Overflow would be great most of the time. Let\u2019s zoom in on the process of stepping through model creation in a debugger and talk about debuggers for deep learning code : In PyTorch, you can use ipdb \u200a\u2014\u200awhich exports functions to access the interactive IPython debugger. In TensorFlow, it\u2019s trickier. TensorFlow separates the process of creating the graph and executing operations in the graph. There are three options you can try: (1) step through the graph creation itself and inspect each tensor layer, (2) step into the training loop and evaluate the tensor layers, or (3) use TensorFlow Debugger (tfdb), which does option 1 and 2 automatically.","title":"Get Your Model To Run"},{"location":"spring2021/lecture-7/#overfit-a-single-batch","text":"After getting your model to run, the next thing you need to do is to overfit a single batch of data . This is a heuristic that can catch an absurd number of bugs. This really means that you want to drive your training error arbitrarily close to 0. There are a few things that can happen when you try to overfit a single batch and it fails: Error goes up : Commonly, this is due to a flip sign somewhere in the loss function/gradient. Error explodes : This is usually a numerical issue but can also be caused by a high learning rate. Error oscillates : You can lower the learning rate and inspect the data for shuffled labels or incorrect data augmentation. Error plateaus : You can increase the learning rate and get rid of regulation. Then you can inspect the loss function and the data pipeline for correctness.","title":"Overfit A Single Batch"},{"location":"spring2021/lecture-7/#compare-to-a-known-result","text":"Once your model overfits in a single batch, there can still be some other issues that cause bugs. The last step here is to compare your results to a known result . So what sort of known results are useful? The most useful results come from an official model implementation evaluated on a similar dataset to yours . You can step through the code in both models line-by-line and ensure your model has the same output. You want to ensure that your model performance is up to par with expectations. If you can\u2019t find an official implementation on a similar dataset, you can compare your approach to results from an official model implementation evaluated on a benchmark dataset . You most definitely want to walk through the code line-by-line and ensure you have the same output. If there is no official implementation of your approach, you can compare it to results from an unofficial model implementation . You can review the code the same as before but with lower confidence (because almost all the unofficial implementations on GitHub have bugs). Then, you can compare to results from a paper with no code (to ensure that your performance is up to par with expectations), results from your model on a benchmark dataset (to make sure your model performs well in a simpler setting), and results from a similar model on a similar dataset (to help you get a general sense of what kind of performance can be expected). An under-rated source of results comes from simple baselines (for example, the average of outputs or linear regression), which can help make sure that your model is learning anything at all. The diagram below neatly summarizes how to implement and debug deep neural networks:","title":"Compare To A Known Result"},{"location":"spring2021/lecture-7/#5-evaluate","text":"","title":"5 - Evaluate"},{"location":"spring2021/lecture-7/#bias-variance-decomposition","text":"To evaluate models and prioritize the next steps in model development, we will apply the bias-variance decomposition. The bias-variance decomposition is the fundamental model fitting tradeoff. In our application, let\u2019s talk more specifically about the formula for bias-variance tradeoff with respect to the test error; this will help us apply the concept more directly to our model\u2019s performance. There are four terms in the formula for test error: Test error = irreducible error + bias + variance + validation overfitting Irreducible error is the baseline error you don\u2019t expect your model to do better. It can be estimated through strong baselines, like human performance. Avoidable bias , a measure of underfitting, is the difference between our train error and irreducible error. Variance , a measure of overfitting, is the difference between validation error and training error. Validation set overfitting is the difference between test error and validation error. Consider the chart of learning curves and errors below. Using the test error formula for bias and variance, we can calculate each component of test error and make decisions based on the value. For example, our avoidable bias is rather low (only 2 points), while the variance is much higher (5 points). With this knowledge, we should prioritize methods of preventing overfitting, like regularization.","title":"Bias-Variance Decomposition"},{"location":"spring2021/lecture-7/#distribution-shift","text":"Clearly, the application of the bias-variance decomposition to the test error has already helped prioritize our next steps for model development. However, until now, we\u2019ve assumed that the samples (training, validation, testing) all come from the same distribution. What if this isn\u2019t the case? In practical ML situations, this distribution shift often cars. In building self-driving cars, a frequent occurrence might be training with samples from one distribution (e.g., daytime driving video) but testing or inferring on samples from a totally different distribution (e.g., night time driving). A simple way of handling this wrinkle in our assumption is to create two validation sets: one from the training distribution and one from the test distribution. This can be helpful even with a very small testing set. If we apply this, we can actually estimate our distribution shift, which is the difference between testing validation error and testing error. This is really useful for practical applications of ML! With this new term, let\u2019s update our test error formula of bias and variance: Test error = irreducible error + bias + variance + distribution shift + validation overfitting","title":"Distribution Shift"},{"location":"spring2021/lecture-7/#6-improve-model-and-data","text":"Using the updated formula from the last section, we\u2019ll be able to decide on and prioritize the right next steps for each iteration of a model. In particular, we\u2019ll follow a specific process (shown below).","title":"6 - Improve Model and Data"},{"location":"spring2021/lecture-7/#step-1-address-underfitting","text":"We\u2019ll start by addressing underfitting (i.e., reducing bias). The first thing to try in this case is to make your model bigger (e.g., add layers, more units per layer). Next, consider regularization, which can prevent a tight fit to your data. Other options are error analysis, choosing a different model architecture (e.g., something more state of the art), tuning hyperparameters, or adding features. Some notes: Choosing different architectures, especially a SOTA one, can be very helpful but is also risky. Bugs are easily introduced in the implementation process. Adding features is uncommon in the deep learning paradigm (vs. traditional machine learning). We usually want the network to learn features of its own accord. If all else fails, it can be beneficial in a practical setting.","title":"Step 1: Address Underfitting"},{"location":"spring2021/lecture-7/#step-2-address-overfitting","text":"After addressing underfitting, move on to solving overfitting. Similarly, there\u2019s a recommended series of methods to try in order. Starting with collecting training data (if possible) is the soundest way to address overfitting, though it can be challenging in certain applications. Next, tactical improvements like normalization, data augmentation, and regularization can help. Following these steps, traditional defaults like tuning hyperparameters, choosing a different architecture, or error analysis are useful. Finally, if overfitting is rather intractable, there\u2019s a series of less recommended steps, such as early stopping, removing features, and reducing model size. Early stopping is a personal choice; the fast.ai community is a strong proponent.","title":"Step 2: Address Overfitting"},{"location":"spring2021/lecture-7/#step-3-address-distribution-shift","text":"After addressing underfitting and overfitting, If there\u2019s a difference between the error on our training validation set vs. our test validation set, we need to address the error caused by the distribution shift. This is a harder problem to solve, so there\u2019s less in our toolkit to apply. Start by looking manually at the errors in the test-validation set. Compare the potential logic behind these errors to the performance in the train-validation set, and use the errors to guide further data collection. Essentially, reason about why your model may be suffering from distribution shift error. This is the most principled way to deal with distribution shift, though it\u2019s the most challenging way practically. If collecting more data to address these errors isn\u2019t possible, try synthesizing data. Additionally, you can try domain adaptation .","title":"Step 3: Address Distribution Shift"},{"location":"spring2021/lecture-7/#error-analysis","text":"Manually evaluating errors to understand model performance is generally a high-yield way of figuring out how to improve the model. Systematically performing this error analysis process and decomposing the error from different error types can help prioritize model improvements. For example, in a self-driving car use case with error types like hard-to-see pedestrians, reflections, and nighttime scenes, decomposing the error contribution of each and where it occurs (train-val vs. test-val) can give rise to a clear set of prioritized action items. See the table for an example of how this error analysis can be effectively structured.","title":"Error Analysis"},{"location":"spring2021/lecture-7/#domain-adaptation","text":"Domain adaptation is a class of techniques that train on a \u201csource\u201d distribution and generalize to another \u201ctarget\u201d using only unlabeled data or limited labeled data. You should use domain adaptation when access to labeled data from the test distribution is limited, but access to relatively similar data is plentiful. There are a few different types of domain adaptation: Supervised domain adaptation : In this case, we have limited data from the target domain to adapt to. Some example applications of the concept include fine-tuning a pre-trained model or adding target data to a training set. Unsupervised domain adaptation : In this case, we have lots of unlabeled data from the target domain. Some techniques you might see are CORAL, domain confusion, and CycleGAN. Practically speaking, supervised domain adaptation can work really well! Unsupervised domain adaptation has a little bit further to go.","title":"Domain Adaptation"},{"location":"spring2021/lecture-7/#step-4-rebalance-datasets","text":"If the test-validation set performance starts to look considerably better than the test performance, you may have overfit the validation set. This commonly occurs with small validation sets or lots of hyperparameter training. If this occurs, resample the validation set from the test distribution and get a fresh estimate of the performance.","title":"Step 4: Rebalance datasets"},{"location":"spring2021/lecture-7/#7-tune-hyperparameters","text":"One of the core challenges in hyperparameter optimization is very basic: which hyperparameters should you tune? As we consider this fundamental question, let\u2019s keep the following in mind: Models are more sensitive to some hyperparameters than others. This means we should focus our efforts on the more impactful hyperparameters. However, which hyperparameters are most important depends heavily on our choice of model. Certain rules of thumbs can help guide our initial thinking. Sensitivity is always relative to default values; if you use good defaults, you might start in a good place! See the following table for a ranked list of hyperparameters and their impact on the model:","title":"7 - Tune Hyperparameters"},{"location":"spring2021/lecture-7/#techniques-for-tuning-hyperparameter-optimization","text":"Now that we know which hyperparameters make the most sense to tune (using rules of thumb), let\u2019s consider the various methods of actually tuning them: Manual Hyperparameter Optimization . Colloquially referred to as Graduate Student Descent, this method works by taking a manual, detailed look at your algorithm, building intuition, and considering which hyperparameters would make the most difference. After figuring out these parameters, you train, evaluate, and guess a better hyperparameter value using your intuition for the algorithm and intelligence. While it may seem archaic, this method combines well with other methods (e.g., setting a range of values for hyperparameters) and has the main benefit of reducing computation time and cost if used skillfully. It can be time-consuming and challenging, but it can be a good starting point. Grid Search . Imagine each of your parameters plotted against each other on a grid, from which you uniformly sample values to test. For each point, you run a training run and evaluate performance. The advantages are that it\u2019s very simple and can often produce good results. However, it\u2019s quite inefficient, as you must run every combination of hyperparameters. It also often requires prior knowledge about the hyperparameters since we must manually set the range of values. Random Search : This method is recommended over grid search. Rather than sampling from the grid of values for the hyperparameter evenly, we\u2019ll choose n points sampled randomly across the grid. Empirically, this method produces better results than grid search. However, the results can be somewhat uninterpretable, with unexpected values in certain hyperparameters returned. Coarse-to-fine Search : Rather than running entirely random runs, we can gradually narrow in on the best hyperparameters through this method. Initially, start by defining a very large range to run a randomized search on. Within the pool of results, you can find N best results and hone in on the hyperparameter values used to generate those samples. As you iteratively perform this method, you can get excellent performance. This doesn\u2019t remove the manual component, as you have to select which range to continuously narrow your search to, but it\u2019s perhaps the most popular method available. Bayesian Hyperparameter Optimization : This is a reasonably sophisticated method, which you can read more about here and here . At a high level, start with a prior estimate of parameter distributions. Subsequently, maintain a probabilistic model of the relationship between hyperparameter values and model performance. As you maintain this model, you toggle between training with hyperparameter values that maximize the expected improvement (per the model) and use training results to update the initial probabilistic model and its expectations. This is a great, hands-off, efficient method to choose hyperparameters. However, these techniques can be quite challenging to implement from scratch. As libraries and infrastructure mature, the integration of these methods into training will become easier. In summary, you should probably start with coarse-to-fine random searches and move to Bayesian methods as your codebase matures and you\u2019re more certain of your model.","title":"Techniques for Tuning Hyperparameter Optimization"},{"location":"spring2021/lecture-7/#8-conclusion","text":"To wrap up this lecture, deep learning troubleshooting and debugging is really hard. It\u2019s difficult to tell if you have a bug because there are many possible sources for the same degradation in performance. Furthermore, the results can be sensitive to small changes in hyper-parameters and dataset makeup. To train bug-free deep learning models, we need to treat building them as an iterative process. If you skipped to the end, the following steps can make this process easier and catch errors as early as possible: Start Simple : Choose the simplest model and data possible. Implement and Debug : Once the model runs, overfit a single batch and reproduce a known result. Evaluate : Apply the bias-variance decomposition to decide what to do next. Tune Hyper-parameters : Use coarse-to-fine random searches to tune the model\u2019s hyper-parameters. Improve Model and Data : Make your model bigger if your model under-fits and add more data and/or regularization if your model over-fits. Here are additional resources that you can go to learn more: Andrew Ng\u2019s \u201c Machine Learning Yearning \u201d book. This Twitter thread from Andrej Karpathy. BYU\u2019s \u201c Practical Advice for Building Deep Neural Networks \u201d blog post.","title":"8 - Conclusion"},{"location":"spring2021/lecture-8/","text":"\u2728Lecture 8: Data Management\u2728 Video Slides PDF Download Notes Notes were taken by James Le and Vishnu Rachakonda One of the best data science articles written in 2019 is \u201c Data science is different now \u201d by Vicki Boykis . Part of the article is a collection of tweets from other data science and machine learning practitioners. 1 - Data Management Overview When we think about what data management for deep learning entails, there might be many different data sources: images on S3, text files on a file system, logs spread across different machines, and maybe even records in a database. At some point, you need to get all of that data over to a local filesystem next to GPUs. The way you will get data over to that trainable format is different for every project and every company. For instance: Maybe you train your images on ImageNet, and all the images are just S3 URLs. Then, all you have to do is download them over to the local filesystem. Maybe you have a bunch of text files that you crawled yourself somewhere. You want to use Spark to process them on a cluster and Pandas data frame to analyze/select subsets that will be used in the local filesystem. Maybe you collect logs and records from your database into a data lake/warehouse (like Snowflake). Then, you process that output and convert them into a trainable format. There are countless possibilities that we are not going to cover completely in this lecture, but here are the key points to remember: Let the data flow through you : You should spend 10x as much time as you want to on exploring the dataset. Data is the best way to improve your overall ML project performance : Instead of trying new architectures or kicking off the hyper-parameter search, adding more data and augmenting the existing dataset will often be the best bang to your buck. Keep It Simple Stupid: We will discuss complex pipelines and new terms, but it\u2019s important to not over-complicate things and make data management a rocket science. 2 - Data Sources So, where do the training data come from? Most deep learning applications require lots of labeled data (with exceptions in applications of reinforcement learning, GANs, and GPT-3). There are publicly available datasets that can serve as a starting point, but there is no competitive advantage of using them. In fact, most companies usually spend a lot of money and time labeling their own data. Data Flywheel Data flywheel is an exciting concept: if you can get your models in front of the users, you can build your products in a mechanism that your users contribute good data back to you and improve the model predictions. This can enable rapid improvement after you get that v1 model out into the real world. Semi-Supervised Learning Semi-supervised learning is a relatively recent learning technique where the training data is autonomously (or automatically) labeled. It is still supervised learning, but the datasets do not need to be manually labeled by a human; but they can be labeled by finding and exploiting the relations (or correlations) between different input signals (that is, input coming from different sensor modalities). A natural advantage and consequence of semi-supervised learning are that this technique can be performed in an online fashion (given that data can be gathered and labeled without human intervention) more easily (with respect to, e.g., supervised learning), where models can be updated or trained entirely from scratch. Therefore, semi-supervised learning should also be well suited for changing environments, changing data, and, in general, changing requirements. For a text example, you can predict the future words from the past words, predict the beginning of a sentence from the end of a sentence, or predict the middle word of a sentence from the words surrounding it. You can even examine whether two sentences occur in the same paragraph in the same corpus of your training data. These are different ways to formulate the problem, where you don\u2019t need to label anything and simply use the data to supervise itself. This technique also applies to vision. Facebook AI recently released a model called SEER trained on 1 billion random images from the Internet. Yet, SEER achieved state-of-the-art accuracy on the ImageNet top-1 prediction task. If you\u2019re interested in learning more about semi-supervised learning, check out: Lilian Weng's \"Self-Supervised Learning\" post Facebook AI\u2019s \u201c Self-Supervised Learning: The Dark Matter Of Intelligence \u201d post Facebook AI\u2019s VISSL library for the SEER algorithm Data Augmentation Recent advances in deep learning models have been largely attributed to the quantity and diversity of data gathered in recent years. Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks. In fact, they are mostly required for training computer vision models. Both Keras and fast.ai provide functions that do this. Data augmentation also applies to other types of data. For tabular data, you can delete some cells to simulate missing data. For text, there are no well-established techniques, but you can replace words with synonyms and change the order of things. For speech and video, you can change speed, insert a pause, mix different sequences, and more. If you\u2019re interested in learning more about data augmentation, check out: Berkeley AI\u2019s \u201c 1000x Faster Data Augmentation \u201d post Edward Ma\u2019s \u201c nlpaug \u201d repository Synthetic Data Related to the concept of data augmentation is synthetic data, an underrated idea that is almost always worth starting with. Synthetic data is data that\u2019s generated programmatically. For example, photorealistic images of objects in arbitrary scenes can be rendered using video game engines or audio generated by a speech synthesis model from the known text. It\u2019s not unlike traditional data augmentation, where crops, flips, rotations, and distortions are used to increase the variety of data that models have to learn from. Synthetically generated data takes those same concepts even further. Most of today\u2019s synthetic data is visual. Tools and techniques developed to create photorealistic graphics in movies and computer games are repurposed to create the training data needed for machine learning. Not only can these rendering engines produce arbitrary numbers of images, but they can also produce annotations too. Bounding boxes, segmentation masks, depth maps, and any other metadata is output right alongside pictures, making it simple to build pipelines that produce their own data. Because samples are generated programmatically along with annotations, synthetic datasets are far cheaper to produce than traditional ones. That means we can create more data and iterate more often to produce better results. Need to add another class to your model? No problem. Need to add another key point to the annotation? Done. This is especially useful for applications in driving and robotics. If you\u2019re interested in learning more about synthetic data, check out: Dropbox\u2019s \u201c Creating A Modern OCR Pipeline Using Computer Vision and Deep Learning \u201d post Andrew Moffat\u2019s \u201c metabrite-receipt-tests \u201d repository Microsoft\u2019s AirSim simulator OpenAI\u2019s \u201c Ingredients For Robotics Research \u201d post 3 - Data Storage Data storage requirements for AI vary widely according to the application and the source material. Datasets in intelligence, defense, medical, science, and geology frequently combine petabyte-scale storage volumes with individual file sizes in the gigabyte range. By contrast, data used in areas such as supply chain analytics and fraud detection are much smaller. There are four building blocks in a data storage system: The filesystem The object storage The database The data lake or data warehouse Filesystem The filesystem is the foundational layer of storage. Its fundamental unit is a \u201cfile\u201d\u200a\u2014\u200awhich can be text or binary, is not versioned, and is easily overwritten. A file system can be as simple as a locally mounted disk containing all the files you need. More advanced options include networked filesystems ( NFS ), which are accessible over the network by multiple machines, and distributed file systems ( HDFS ) which are stored and accessed over multiple machines. The plots above display hard-drive speeds for SATA hard drive, SATA SSD, and NVMe SSD. The left plot shows the sustained throughput in MBps (how much information to copy a file): The latest iteration of hard drive technology (NVMe) is 6-10x more powerful than older iterations. The right plot shows the seek time in milliseconds (how long it takes to go to a file on disk): The NVMe is 25-30x faster than the old-school ones. What format should we store data in? For binary data (images, audios, videos), just files are enough. In Tensorflow, you have the TFRecord format to batch binary files, which does not seem to be necessary with the NVMe hard drives. For large tabular and text data, you have two choices: HDF5 is powerful but bloated and declining. Parquet is widespread and recommended. Feather is an up-and-coming open-source option powered by Apache Arrow. Both Tensorflow and PyTorch provide their native dataset class interfaces ( tf.data and PyTorch DataLoader ). Object Storage Object storage is an API over the filesystem that allows users to use a command on files (GET, PUT, DELETE) to a service without worrying where they are actually stored. Its fundamental unit is an \u201cobject,\u201d\u200a \u200awhich is usually binary (images, sound files\u2026). Object storage can be built with data versioning and data redundancy into the API. It is not as fast as local files but fast enough within the cloud. Database A database is a persistent, fast, scalable storage and retrieval of structured data. Its fundamental unit is a \u201crow\u201d (unique IDs, references to other rows, values in columns). Databases are also known for online transaction processing (OLTP). The mental model here is that everything is actually in memory, but the software ensures that everything is logged to disk and never lost. Databases are not built for binary data, so you must store the references (i.e., S3 URLs) instead. Here are our recommendations: PostgreSQL is the right choice most of the time, thanks to the support of unstructured JSON. SQLite is perfectly good for small projects. \u201cNoSQL\u201d was a big craze in the 2010s (like MongoDB ). However, they are not as fast as the relational database and also have consistency issues frequently. Redis is handy when you need a simple key-value store. Data Warehouse A data warehouse is a structured aggregation of data for analysis, known as online analytical processing (OLAP). Another acronym that you might have heard of is ETL ( Extract, Transform, Load ). The idea here is to extract data from data sources, transform the data into a common schema, and load the schema into the data warehouse. You can load the subset from the warehouse that you need and generate reports or run analytical queries. Well-known enterprise options in the market are Google BigQuery , Amazon Redshift , and Snowflake . SQL and DataFrames Most data solutions use SQL as the interface to the data, except for some (like Databricks) that use DataFrames . SQL is the standard interface for structured data. But in the Python ecosystem, Pandas is the main DataFrame. Our advice is to become fluent in both. Data Lake A data lake is the unstructured aggregation of data from multiple sources (databases, logs, expensive data transformations). It operates under the concept of ELT ( Extract, Load, Transform ) by dumping everything in the lake and transforming the data for specific needs later. Data \u201cLakehouse\u201d The current trend in the field is to combine data warehouses and data lakes in the same suite. The Databricks Lakehouse Platform is both a warehouse and a lake, operated as an open-source project called Delta Lake . You can store both structured and unstructured data in the platform and use them for analytics workloads and machine learning engines. What Goes Where? Binary data (images, sound files, compressed texts) are stored as objects . Metadata (labels, user activity) is stored in a database . If we need features that are not obtainable from the database ( logs ), we would want to set up a data lake and a process to aggregate the data required. At training time , we need to copy the necessary data to the filesystem on a fast drive. A lot is going on within the data management tooling and infrastructure. We recommend looking at a16z\u2019s \u201c Emerging Architectures For Modern Data Infrastructure \u201d article to get a broad look into this ecosystem. A highly recommended resource is Martin Kleppmann\u2019s book \u201c Designing Data-Intensive Applications ,\u201d\u200a \u200awhich provides excellent coverage of tools and approaches to build reliable, scalable, and maintainable data storage systems. 4 - Data Processing Data Dependencies Let\u2019s look at a motivational example of training a photo popularity predictor every night. For each photo, the training data must include these components: Metadata (such as posting time, title, location) that is in the database. Some features of the user (such as how many times they logged in today) that need to be computed from logs. Outputs of photo classifiers (such as content, style) that can be obtained after running the classifiers. The idea is that we have different sources of data, and they have different dependencies. The big hurdle here is that some tasks can\u2019t be started until other tasks are finished. Finishing a task should \u201ckick-off\u201d its dependencies. The simplest thing we can do is a \u201cMakefile\u201d to specify what action(s) depend on. But here are some limitations to this approach: What if re-computation needs to depend on content, not on a date? What if the dependencies are not files but disparate programs and databases? What if the work needs to be spread over multiple machines? What if many dependency graphs are executing all at once, with shared dependencies? MapReduce The old-school big data solutions to this are Hadoop and Apache Spark . These are MapReduce implementations, where you launch different tasks that each take a bit of the data (Map) and reduce their outputs into a single output (Reduce). Both Hadoop and Spark can run data processing operations and simple ML models on commodity hardware, with tricks to speed things up. In the modern environment, you can\u2019t run an ML model (in PyTorch or TensorFlow) as part of running a Spark job (unless that model itself is programmed in Spark). That\u2019s when you need a workflow management system like Apache Airflow . DAG In Airflow, a workflow is defined as a collection of tasks with directional dependencies, basically a directed acyclic graph (DAG). Each node in the graph is a task, and the edges define dependencies among the tasks. Tasks belong to two categories: (1) operators that execute some operation and (2) sensors that check for the state of a process or a data structure. The main components of Airflow include: (1) a metadata database that stores the state of tasks and workflows, (2) a scheduler that uses the DAGs definitions together with the state of tasks in the metadata database to decide what needs to be executed, and (3) an executor that determines which worker will execute each task. Besides Airflow, here are other notable solutions: Apache Beam : The TensorFlow team uses Apache Beam to generate big datasets and run those processing steps on Google Cloud Dataflow (a cloud orchestrator). Prefect : A similar idea to Airflow, Prefect is a Python framework that makes it easy to combine tasks into workflows, then deploy, schedule, and monitor their execution through the Prefect UI or API. dbt : dbt provides this data processing ability in SQL (called \u201canalytics engineering.\u201d) Dagster : Dagster is another data orchestrator for ML, analytics, and ETL. You can test locally and run anywhere with a unified view of data pipelines and assets. 5 - Feature Store Feature stores were first popularized by the ML team at Uber as part of their Michelangelo platform. Traditionally, ML systems are divided into two portions, offline processing and online processing. For the initial work of modeling, data that is generally static, perhaps stored in a data lake. Using some preprocessing methods (usually in SQL or Spark), data, which could be logfiles, requests, etc., are converted into features used to develop and train the model. The end result of this process is a model trained on a static sample of the data. This is an offline process . In contrast, the process of performing inference (e.g., Uber\u2019s need to return ride prices in real-time) often works with real-time data in an online process fashion. From a technology standpoint, whereas the offline use case might involve a data lake and Spark/SQL, the online processing use case involves technologies like Kafka and Cassandra that support speedier processing of creating or accessing the features required to perform inference. This difference in how features need to be created and accessed is a natural place for bugs to crop up. Harmonization of the online and offline processes would reduce bugs, so the Uber team, amongst others, introduced the concept of features stores to do just that. Members of the Uber team developed Tecton , a feature store company, which is one option to implement this system. An open-source alternative is Feast . To summarize, Tecton offers a handy definition of what a feature store is: \u201c an ML-specific data system that runs data pipelines that transform raw data into feature values, stores and manages the feature data itself, and serves feature data consistently for training and inference purposes.\u201d A word of caution: don\u2019t over-engineer your system according to what others are doing. It\u2019s easy to wrap yourself up in adopting many tools and systems that aren\u2019t as optimal as their publicity may make them seem. Work with the tools you have first! For an interesting example of this, look at how \u201c command-line tools can be 235x faster than your Hadoop cluster \u201d. 6 - Data Exploration The objective of data exploration is to understand and visualize the nature of the data you\u2019re modeling. Pandas is the Python workhorse of data visualization. It\u2019s highly recommended to be familiar with it. Dask is an alternative that can speed up data processing for large datasets that Pandas cannot handle through parallelization. Similarly, RAPIDS speeds up large dataset processing, though it does through the use of GPUs. 7 - Data Labeling Effective data labeling is a core ingredient of production machine learning systems. Most data labeling platforms have a standard set of features: the ability to generate bounding boxes, segmentations, key points, class assignments, etc. The crucial objective is agreeing on what makes a good annotation and training annotators accordingly. To avoid annotator error cropping up, write clear guidelines that clarify rules for edge cases and high-quality annotations. One way to acquire the material needed to write such a guide is to start by annotating yourself. As you generate labels, ensure the quality of the annotations holds up across the annotator base. Some participants will be more reliable than others. To develop an annotator base, there are a few options. Sources of Labor One option is to hire your own annotators , which can help with the speed and quality of annotations. This, however, can be expensive and difficult to scale. Another option is to crowdsource labels via a platform like Amazon Mechanical Turk, which is fast and cheap to set up, but for which the quality can be poorer. \u2026or full-service data labeling companies . Service Companies There are entire service companies that focus on data labeling that you can hire. Hiring such a company makes a great deal of sense, considering the time, labor, and software investment needed to label well at scale. To figure out the best data labeling company, start by annotating some gold standard data yourself. Then, contact and evaluate several companies on their value and a sample labeling task. Some companies in this space are FigureEight , Scale.ai , Labelbox , and Supervisely . Software If the costs of a full-service data labeling company are prohibitive, pure-play labeling software can be an option. Label Studio is a friendly open-source platform for this. New concepts to make labeling more strategic and efficient are coming to the fore. Aquarium helps you explore your data extensively and map the appropriate labeling strategy for classes that may be less prevalent or performant. Snorkel.ai offers a platform that incorporates weak supervision, which automatically labels data points based on heuristics and human feedback. In summary, if you can afford not to label, don\u2019t; get a full-service company to take care of it. Failing that, try to use existing software and a part-time annotator base work (in lieu of a crowdsourced workforce). 8 - Data Versioning Data versioning is important because machine learning models are part code and part data. If the data isn\u2019t versioned, the system isn\u2019t fully versioned! There are four levels to data versioning, which is similar to code versioning: Level 0: No versioning . All data lives on a filesystem, in S3, and/or in a database. The problem arises most acutely in this paradigm, as deployed ML systems (whose code may be versioned) can quickly become divorced from their corresponding data. Furthermore, reverting to older versions will be challenging. Level 1: Storing a snapshot of everything at training time . This works and can help you revert, but it\u2019s very hacky. Rather than doing this entire process manually, let\u2019s try to version automatically. Level 2: Versioned as a mix of assets and code . You store the large files with unique IDs in S3, with corresponding reference JSON versioned with code. You should avoid storing the data directly in the repository, as the metadata itself can get pretty large. Using git-lfs lets you store them just as easily as code. The git signature + of the raw data file fully defines a model\u2019s data and code. Level 3: Specialized solutions for version data . You should avoid them until you can identify their unique value add to your project. Some options here are DVC are Pachyderm . DVC has a Git-like workflow worth taking a closer look at. Dolt versions databases, if that\u2019s your need. 9 - Data Privacy Increasingly, unfettered access to data for machine learning is less desirable and prevalent. This is especially true in regulated industries like healthcare and finance. To address such challenges, researchers are developing new data privacy techniques. Federated learning trains a global model on several local devices without ever acquiring global access to the data. Federated learning is still research-use only due to these issues: (1) sending model updates can be expensive, (2) the depth of anonymization is not clear, and (3) system heterogeneity when it comes to training is unacceptably high. Another research area is differential privacy , which tries to aggregate data in ways that prevent identification. Finally, learning on encrypted data has potential. Most data privacy efforts are research-focused, as the tooling is not yet mature.","title":"\u2728Lecture 8: Data Management\u2728"},{"location":"spring2021/lecture-8/#lecture-8-data-management","text":"","title":"\u2728Lecture 8: Data Management\u2728"},{"location":"spring2021/lecture-8/#video","text":"","title":"Video"},{"location":"spring2021/lecture-8/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-8/#notes","text":"Notes were taken by James Le and Vishnu Rachakonda One of the best data science articles written in 2019 is \u201c Data science is different now \u201d by Vicki Boykis . Part of the article is a collection of tweets from other data science and machine learning practitioners.","title":"Notes"},{"location":"spring2021/lecture-8/#1-data-management-overview","text":"When we think about what data management for deep learning entails, there might be many different data sources: images on S3, text files on a file system, logs spread across different machines, and maybe even records in a database. At some point, you need to get all of that data over to a local filesystem next to GPUs. The way you will get data over to that trainable format is different for every project and every company. For instance: Maybe you train your images on ImageNet, and all the images are just S3 URLs. Then, all you have to do is download them over to the local filesystem. Maybe you have a bunch of text files that you crawled yourself somewhere. You want to use Spark to process them on a cluster and Pandas data frame to analyze/select subsets that will be used in the local filesystem. Maybe you collect logs and records from your database into a data lake/warehouse (like Snowflake). Then, you process that output and convert them into a trainable format. There are countless possibilities that we are not going to cover completely in this lecture, but here are the key points to remember: Let the data flow through you : You should spend 10x as much time as you want to on exploring the dataset. Data is the best way to improve your overall ML project performance : Instead of trying new architectures or kicking off the hyper-parameter search, adding more data and augmenting the existing dataset will often be the best bang to your buck. Keep It Simple Stupid: We will discuss complex pipelines and new terms, but it\u2019s important to not over-complicate things and make data management a rocket science.","title":"1 - Data Management Overview"},{"location":"spring2021/lecture-8/#2-data-sources","text":"So, where do the training data come from? Most deep learning applications require lots of labeled data (with exceptions in applications of reinforcement learning, GANs, and GPT-3). There are publicly available datasets that can serve as a starting point, but there is no competitive advantage of using them. In fact, most companies usually spend a lot of money and time labeling their own data.","title":"2 - Data Sources"},{"location":"spring2021/lecture-8/#data-flywheel","text":"Data flywheel is an exciting concept: if you can get your models in front of the users, you can build your products in a mechanism that your users contribute good data back to you and improve the model predictions. This can enable rapid improvement after you get that v1 model out into the real world.","title":"Data Flywheel"},{"location":"spring2021/lecture-8/#semi-supervised-learning","text":"Semi-supervised learning is a relatively recent learning technique where the training data is autonomously (or automatically) labeled. It is still supervised learning, but the datasets do not need to be manually labeled by a human; but they can be labeled by finding and exploiting the relations (or correlations) between different input signals (that is, input coming from different sensor modalities). A natural advantage and consequence of semi-supervised learning are that this technique can be performed in an online fashion (given that data can be gathered and labeled without human intervention) more easily (with respect to, e.g., supervised learning), where models can be updated or trained entirely from scratch. Therefore, semi-supervised learning should also be well suited for changing environments, changing data, and, in general, changing requirements. For a text example, you can predict the future words from the past words, predict the beginning of a sentence from the end of a sentence, or predict the middle word of a sentence from the words surrounding it. You can even examine whether two sentences occur in the same paragraph in the same corpus of your training data. These are different ways to formulate the problem, where you don\u2019t need to label anything and simply use the data to supervise itself. This technique also applies to vision. Facebook AI recently released a model called SEER trained on 1 billion random images from the Internet. Yet, SEER achieved state-of-the-art accuracy on the ImageNet top-1 prediction task. If you\u2019re interested in learning more about semi-supervised learning, check out: Lilian Weng's \"Self-Supervised Learning\" post Facebook AI\u2019s \u201c Self-Supervised Learning: The Dark Matter Of Intelligence \u201d post Facebook AI\u2019s VISSL library for the SEER algorithm","title":"Semi-Supervised Learning"},{"location":"spring2021/lecture-8/#data-augmentation","text":"Recent advances in deep learning models have been largely attributed to the quantity and diversity of data gathered in recent years. Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks. In fact, they are mostly required for training computer vision models. Both Keras and fast.ai provide functions that do this. Data augmentation also applies to other types of data. For tabular data, you can delete some cells to simulate missing data. For text, there are no well-established techniques, but you can replace words with synonyms and change the order of things. For speech and video, you can change speed, insert a pause, mix different sequences, and more. If you\u2019re interested in learning more about data augmentation, check out: Berkeley AI\u2019s \u201c 1000x Faster Data Augmentation \u201d post Edward Ma\u2019s \u201c nlpaug \u201d repository","title":"Data Augmentation"},{"location":"spring2021/lecture-8/#synthetic-data","text":"Related to the concept of data augmentation is synthetic data, an underrated idea that is almost always worth starting with. Synthetic data is data that\u2019s generated programmatically. For example, photorealistic images of objects in arbitrary scenes can be rendered using video game engines or audio generated by a speech synthesis model from the known text. It\u2019s not unlike traditional data augmentation, where crops, flips, rotations, and distortions are used to increase the variety of data that models have to learn from. Synthetically generated data takes those same concepts even further. Most of today\u2019s synthetic data is visual. Tools and techniques developed to create photorealistic graphics in movies and computer games are repurposed to create the training data needed for machine learning. Not only can these rendering engines produce arbitrary numbers of images, but they can also produce annotations too. Bounding boxes, segmentation masks, depth maps, and any other metadata is output right alongside pictures, making it simple to build pipelines that produce their own data. Because samples are generated programmatically along with annotations, synthetic datasets are far cheaper to produce than traditional ones. That means we can create more data and iterate more often to produce better results. Need to add another class to your model? No problem. Need to add another key point to the annotation? Done. This is especially useful for applications in driving and robotics. If you\u2019re interested in learning more about synthetic data, check out: Dropbox\u2019s \u201c Creating A Modern OCR Pipeline Using Computer Vision and Deep Learning \u201d post Andrew Moffat\u2019s \u201c metabrite-receipt-tests \u201d repository Microsoft\u2019s AirSim simulator OpenAI\u2019s \u201c Ingredients For Robotics Research \u201d post","title":"Synthetic Data"},{"location":"spring2021/lecture-8/#3-data-storage","text":"Data storage requirements for AI vary widely according to the application and the source material. Datasets in intelligence, defense, medical, science, and geology frequently combine petabyte-scale storage volumes with individual file sizes in the gigabyte range. By contrast, data used in areas such as supply chain analytics and fraud detection are much smaller. There are four building blocks in a data storage system: The filesystem The object storage The database The data lake or data warehouse","title":"3 - Data Storage"},{"location":"spring2021/lecture-8/#filesystem","text":"The filesystem is the foundational layer of storage. Its fundamental unit is a \u201cfile\u201d\u200a\u2014\u200awhich can be text or binary, is not versioned, and is easily overwritten. A file system can be as simple as a locally mounted disk containing all the files you need. More advanced options include networked filesystems ( NFS ), which are accessible over the network by multiple machines, and distributed file systems ( HDFS ) which are stored and accessed over multiple machines. The plots above display hard-drive speeds for SATA hard drive, SATA SSD, and NVMe SSD. The left plot shows the sustained throughput in MBps (how much information to copy a file): The latest iteration of hard drive technology (NVMe) is 6-10x more powerful than older iterations. The right plot shows the seek time in milliseconds (how long it takes to go to a file on disk): The NVMe is 25-30x faster than the old-school ones. What format should we store data in? For binary data (images, audios, videos), just files are enough. In Tensorflow, you have the TFRecord format to batch binary files, which does not seem to be necessary with the NVMe hard drives. For large tabular and text data, you have two choices: HDF5 is powerful but bloated and declining. Parquet is widespread and recommended. Feather is an up-and-coming open-source option powered by Apache Arrow. Both Tensorflow and PyTorch provide their native dataset class interfaces ( tf.data and PyTorch DataLoader ).","title":"Filesystem"},{"location":"spring2021/lecture-8/#object-storage","text":"Object storage is an API over the filesystem that allows users to use a command on files (GET, PUT, DELETE) to a service without worrying where they are actually stored. Its fundamental unit is an \u201cobject,\u201d\u200a \u200awhich is usually binary (images, sound files\u2026). Object storage can be built with data versioning and data redundancy into the API. It is not as fast as local files but fast enough within the cloud.","title":"Object Storage"},{"location":"spring2021/lecture-8/#database","text":"A database is a persistent, fast, scalable storage and retrieval of structured data. Its fundamental unit is a \u201crow\u201d (unique IDs, references to other rows, values in columns). Databases are also known for online transaction processing (OLTP). The mental model here is that everything is actually in memory, but the software ensures that everything is logged to disk and never lost. Databases are not built for binary data, so you must store the references (i.e., S3 URLs) instead. Here are our recommendations: PostgreSQL is the right choice most of the time, thanks to the support of unstructured JSON. SQLite is perfectly good for small projects. \u201cNoSQL\u201d was a big craze in the 2010s (like MongoDB ). However, they are not as fast as the relational database and also have consistency issues frequently. Redis is handy when you need a simple key-value store.","title":"Database"},{"location":"spring2021/lecture-8/#data-warehouse","text":"A data warehouse is a structured aggregation of data for analysis, known as online analytical processing (OLAP). Another acronym that you might have heard of is ETL ( Extract, Transform, Load ). The idea here is to extract data from data sources, transform the data into a common schema, and load the schema into the data warehouse. You can load the subset from the warehouse that you need and generate reports or run analytical queries. Well-known enterprise options in the market are Google BigQuery , Amazon Redshift , and Snowflake .","title":"Data Warehouse"},{"location":"spring2021/lecture-8/#sql-and-dataframes","text":"Most data solutions use SQL as the interface to the data, except for some (like Databricks) that use DataFrames . SQL is the standard interface for structured data. But in the Python ecosystem, Pandas is the main DataFrame. Our advice is to become fluent in both.","title":"SQL and DataFrames"},{"location":"spring2021/lecture-8/#data-lake","text":"A data lake is the unstructured aggregation of data from multiple sources (databases, logs, expensive data transformations). It operates under the concept of ELT ( Extract, Load, Transform ) by dumping everything in the lake and transforming the data for specific needs later.","title":"Data Lake"},{"location":"spring2021/lecture-8/#data-lakehouse","text":"The current trend in the field is to combine data warehouses and data lakes in the same suite. The Databricks Lakehouse Platform is both a warehouse and a lake, operated as an open-source project called Delta Lake . You can store both structured and unstructured data in the platform and use them for analytics workloads and machine learning engines.","title":"Data \u201cLakehouse\u201d"},{"location":"spring2021/lecture-8/#what-goes-where","text":"Binary data (images, sound files, compressed texts) are stored as objects . Metadata (labels, user activity) is stored in a database . If we need features that are not obtainable from the database ( logs ), we would want to set up a data lake and a process to aggregate the data required. At training time , we need to copy the necessary data to the filesystem on a fast drive. A lot is going on within the data management tooling and infrastructure. We recommend looking at a16z\u2019s \u201c Emerging Architectures For Modern Data Infrastructure \u201d article to get a broad look into this ecosystem. A highly recommended resource is Martin Kleppmann\u2019s book \u201c Designing Data-Intensive Applications ,\u201d\u200a \u200awhich provides excellent coverage of tools and approaches to build reliable, scalable, and maintainable data storage systems.","title":"What Goes Where?"},{"location":"spring2021/lecture-8/#4-data-processing","text":"","title":"4 - Data Processing"},{"location":"spring2021/lecture-8/#data-dependencies","text":"Let\u2019s look at a motivational example of training a photo popularity predictor every night. For each photo, the training data must include these components: Metadata (such as posting time, title, location) that is in the database. Some features of the user (such as how many times they logged in today) that need to be computed from logs. Outputs of photo classifiers (such as content, style) that can be obtained after running the classifiers. The idea is that we have different sources of data, and they have different dependencies. The big hurdle here is that some tasks can\u2019t be started until other tasks are finished. Finishing a task should \u201ckick-off\u201d its dependencies. The simplest thing we can do is a \u201cMakefile\u201d to specify what action(s) depend on. But here are some limitations to this approach: What if re-computation needs to depend on content, not on a date? What if the dependencies are not files but disparate programs and databases? What if the work needs to be spread over multiple machines? What if many dependency graphs are executing all at once, with shared dependencies?","title":"Data Dependencies"},{"location":"spring2021/lecture-8/#mapreduce","text":"The old-school big data solutions to this are Hadoop and Apache Spark . These are MapReduce implementations, where you launch different tasks that each take a bit of the data (Map) and reduce their outputs into a single output (Reduce). Both Hadoop and Spark can run data processing operations and simple ML models on commodity hardware, with tricks to speed things up. In the modern environment, you can\u2019t run an ML model (in PyTorch or TensorFlow) as part of running a Spark job (unless that model itself is programmed in Spark). That\u2019s when you need a workflow management system like Apache Airflow .","title":"MapReduce"},{"location":"spring2021/lecture-8/#dag","text":"In Airflow, a workflow is defined as a collection of tasks with directional dependencies, basically a directed acyclic graph (DAG). Each node in the graph is a task, and the edges define dependencies among the tasks. Tasks belong to two categories: (1) operators that execute some operation and (2) sensors that check for the state of a process or a data structure. The main components of Airflow include: (1) a metadata database that stores the state of tasks and workflows, (2) a scheduler that uses the DAGs definitions together with the state of tasks in the metadata database to decide what needs to be executed, and (3) an executor that determines which worker will execute each task. Besides Airflow, here are other notable solutions: Apache Beam : The TensorFlow team uses Apache Beam to generate big datasets and run those processing steps on Google Cloud Dataflow (a cloud orchestrator). Prefect : A similar idea to Airflow, Prefect is a Python framework that makes it easy to combine tasks into workflows, then deploy, schedule, and monitor their execution through the Prefect UI or API. dbt : dbt provides this data processing ability in SQL (called \u201canalytics engineering.\u201d) Dagster : Dagster is another data orchestrator for ML, analytics, and ETL. You can test locally and run anywhere with a unified view of data pipelines and assets.","title":"DAG"},{"location":"spring2021/lecture-8/#5-feature-store","text":"Feature stores were first popularized by the ML team at Uber as part of their Michelangelo platform. Traditionally, ML systems are divided into two portions, offline processing and online processing. For the initial work of modeling, data that is generally static, perhaps stored in a data lake. Using some preprocessing methods (usually in SQL or Spark), data, which could be logfiles, requests, etc., are converted into features used to develop and train the model. The end result of this process is a model trained on a static sample of the data. This is an offline process . In contrast, the process of performing inference (e.g., Uber\u2019s need to return ride prices in real-time) often works with real-time data in an online process fashion. From a technology standpoint, whereas the offline use case might involve a data lake and Spark/SQL, the online processing use case involves technologies like Kafka and Cassandra that support speedier processing of creating or accessing the features required to perform inference. This difference in how features need to be created and accessed is a natural place for bugs to crop up. Harmonization of the online and offline processes would reduce bugs, so the Uber team, amongst others, introduced the concept of features stores to do just that. Members of the Uber team developed Tecton , a feature store company, which is one option to implement this system. An open-source alternative is Feast . To summarize, Tecton offers a handy definition of what a feature store is: \u201c an ML-specific data system that runs data pipelines that transform raw data into feature values, stores and manages the feature data itself, and serves feature data consistently for training and inference purposes.\u201d A word of caution: don\u2019t over-engineer your system according to what others are doing. It\u2019s easy to wrap yourself up in adopting many tools and systems that aren\u2019t as optimal as their publicity may make them seem. Work with the tools you have first! For an interesting example of this, look at how \u201c command-line tools can be 235x faster than your Hadoop cluster \u201d.","title":"5 - Feature Store"},{"location":"spring2021/lecture-8/#6-data-exploration","text":"The objective of data exploration is to understand and visualize the nature of the data you\u2019re modeling. Pandas is the Python workhorse of data visualization. It\u2019s highly recommended to be familiar with it. Dask is an alternative that can speed up data processing for large datasets that Pandas cannot handle through parallelization. Similarly, RAPIDS speeds up large dataset processing, though it does through the use of GPUs.","title":"6 - Data Exploration"},{"location":"spring2021/lecture-8/#7-data-labeling","text":"Effective data labeling is a core ingredient of production machine learning systems. Most data labeling platforms have a standard set of features: the ability to generate bounding boxes, segmentations, key points, class assignments, etc. The crucial objective is agreeing on what makes a good annotation and training annotators accordingly. To avoid annotator error cropping up, write clear guidelines that clarify rules for edge cases and high-quality annotations. One way to acquire the material needed to write such a guide is to start by annotating yourself. As you generate labels, ensure the quality of the annotations holds up across the annotator base. Some participants will be more reliable than others. To develop an annotator base, there are a few options.","title":"7 - Data Labeling"},{"location":"spring2021/lecture-8/#sources-of-labor","text":"One option is to hire your own annotators , which can help with the speed and quality of annotations. This, however, can be expensive and difficult to scale. Another option is to crowdsource labels via a platform like Amazon Mechanical Turk, which is fast and cheap to set up, but for which the quality can be poorer. \u2026or full-service data labeling companies .","title":"Sources of Labor"},{"location":"spring2021/lecture-8/#service-companies","text":"There are entire service companies that focus on data labeling that you can hire. Hiring such a company makes a great deal of sense, considering the time, labor, and software investment needed to label well at scale. To figure out the best data labeling company, start by annotating some gold standard data yourself. Then, contact and evaluate several companies on their value and a sample labeling task. Some companies in this space are FigureEight , Scale.ai , Labelbox , and Supervisely .","title":"Service Companies"},{"location":"spring2021/lecture-8/#software","text":"If the costs of a full-service data labeling company are prohibitive, pure-play labeling software can be an option. Label Studio is a friendly open-source platform for this. New concepts to make labeling more strategic and efficient are coming to the fore. Aquarium helps you explore your data extensively and map the appropriate labeling strategy for classes that may be less prevalent or performant. Snorkel.ai offers a platform that incorporates weak supervision, which automatically labels data points based on heuristics and human feedback. In summary, if you can afford not to label, don\u2019t; get a full-service company to take care of it. Failing that, try to use existing software and a part-time annotator base work (in lieu of a crowdsourced workforce).","title":"Software"},{"location":"spring2021/lecture-8/#8-data-versioning","text":"Data versioning is important because machine learning models are part code and part data. If the data isn\u2019t versioned, the system isn\u2019t fully versioned! There are four levels to data versioning, which is similar to code versioning: Level 0: No versioning . All data lives on a filesystem, in S3, and/or in a database. The problem arises most acutely in this paradigm, as deployed ML systems (whose code may be versioned) can quickly become divorced from their corresponding data. Furthermore, reverting to older versions will be challenging. Level 1: Storing a snapshot of everything at training time . This works and can help you revert, but it\u2019s very hacky. Rather than doing this entire process manually, let\u2019s try to version automatically. Level 2: Versioned as a mix of assets and code . You store the large files with unique IDs in S3, with corresponding reference JSON versioned with code. You should avoid storing the data directly in the repository, as the metadata itself can get pretty large. Using git-lfs lets you store them just as easily as code. The git signature + of the raw data file fully defines a model\u2019s data and code. Level 3: Specialized solutions for version data . You should avoid them until you can identify their unique value add to your project. Some options here are DVC are Pachyderm . DVC has a Git-like workflow worth taking a closer look at. Dolt versions databases, if that\u2019s your need.","title":"8 - Data Versioning"},{"location":"spring2021/lecture-8/#9-data-privacy","text":"Increasingly, unfettered access to data for machine learning is less desirable and prevalent. This is especially true in regulated industries like healthcare and finance. To address such challenges, researchers are developing new data privacy techniques. Federated learning trains a global model on several local devices without ever acquiring global access to the data. Federated learning is still research-use only due to these issues: (1) sending model updates can be expensive, (2) the depth of anonymization is not clear, and (3) system heterogeneity when it comes to training is unacceptably high. Another research area is differential privacy , which tries to aggregate data in ways that prevent identification. Finally, learning on encrypted data has potential. Most data privacy efforts are research-focused, as the tooling is not yet mature.","title":"9 - Data Privacy"},{"location":"spring2021/lecture-9/","text":"\u2728Lecture 9: AI Ethics\u2728 Video Slides PDF Download Notes","title":"\u2728Lecture 9: AI Ethics\u2728"},{"location":"spring2021/lecture-9/#lecture-9-ai-ethics","text":"","title":"\u2728Lecture 9: AI Ethics\u2728"},{"location":"spring2021/lecture-9/#video","text":"","title":"Video"},{"location":"spring2021/lecture-9/#slides","text":"PDF Download","title":"Slides"},{"location":"spring2021/lecture-9/#notes","text":"","title":"Notes"},{"location":"spring2021/notebook-1/","text":"Notebook: Coding a neural net Video In this video, we code a neural network from scratch. You'll get familiar with the Google Colab environment, create a simple linear regression model using only Numpy, and build a multi-layer perception regression model using NumPy, PyTorch, and Keras. 0:30\u200b - Colab Notebook 101 5:30\u200b - Numerical computing via NumPy 10:15\u200b - Plotting via Matplotlib 11:33\u200b - Basic regression with a linear model 24:30\u200b - Basic regression with a multi-layer perceptron Follow Along Google Colab","title":"Notebook: Coding a neural net"},{"location":"spring2021/notebook-1/#notebook-coding-a-neural-net","text":"","title":"Notebook: Coding a neural net"},{"location":"spring2021/notebook-1/#video","text":"In this video, we code a neural network from scratch. You'll get familiar with the Google Colab environment, create a simple linear regression model using only Numpy, and build a multi-layer perception regression model using NumPy, PyTorch, and Keras. 0:30\u200b - Colab Notebook 101 5:30\u200b - Numerical computing via NumPy 10:15\u200b - Plotting via Matplotlib 11:33\u200b - Basic regression with a linear model 24:30\u200b - Basic regression with a multi-layer perceptron","title":"Video"},{"location":"spring2021/notebook-1/#follow-along","text":"Google Colab","title":"Follow Along"},{"location":"spring2021/projects/","text":"","title":"Projects"},{"location":"spring2021/synchronous/","text":"Synchronous Online Course For those of you who signed up, in addition to the lecture and lab materials released publicly, there are some major extras: Slack workspace for learners, instructors, and teaching assistants Weekly graded assignment Capstone project reviewed by peers and staff Certificate of completion How do I know if I am in this course? If you registered and received an email receipt from Stripe, you're in, and should have been added to our Slack workspace on February 1. Please email us if you have a Stripe receipt but aren't in our Slack. Teaching Assistants This course is only possible with the support of our amazing TAs: Head TA James Le runs Data Relations for Superb AI and contributes to Data Journalism for Snorkel AI, after getting an MS in Recommendation Systems at RIT. Daniel Cooper is a machine learning engineer at QuantumWork, SaaS for recruiters. Han Lee is a Senior Data Scientist at WalletHub. Prior to that, he worked on various DS, MLE, and quant roles. Previously, he co-managed TEFQX . Nadia Ahmed is a machine learning researcher with The Frontier Development Lab and Trillium Technologies in remote sensing for severe weather and flood events. Andrew Mendez is a Senior Machine Learning Engineer at Clarifai, developing large scale computer vision and machine learning systems for the public sector. Previously he was a ML Engineer at CACI. Vishnu Rachakonda is a Machine Learning Engineer at Tesseract Health, a retinal imaging company, where he builds machine learning models for workflow augmentation and diagnostics in on-device and cloud use cases. Chester Chen is the Director of Data Science Engineering at GoPro. He also founded the SF Big Analytics Meetup. Schedule While we post lectures once a week starting February 1, the first four weeks are review lectures -- stuff you should already know from other courses. On March 1, we get to the Full Stack content, and you will begin doing weekly assignments, discussing in Slack, and thinking about their course project. Logistics All learners, instructors, and TAs will be part of a Slack workspace. The Slack community is a crucial part of the course: a place to meet each other, post helpful links, share experiences, ask and answer questions. On Monday, we post the lecture and lab videos for you to watch. Post questions, ideas, articles in Slack as you view the materials. On Thursday, we go live on Zoom to discuss the posted questions and ideas. We have two 30-min slots: 9am and 6pm Pacific Time. We will send everyone a Google Calendar invite with the Zoom meeting information. You have until Friday night to finish the assignment via Gradescope, which will be graded by next Tuesday, so that you have prompt feedback. Labs are not graded and can be done on your own. Projects The final project is the most important as well as the most fun part of the course. You can pair up or work individually. The project can involve any part of the full stack of deep learning, and should take you roughly 40 hours per person, over 5 weeks. Projects will be presented as five-minute videos and associated reports, and open sourcing the code is highly encouraged. All projects will be posted for peer and staff review. The best projects will be awarded and publicized by Full Stack Deep Learning. If you want to find a partner, please post in the #spring2021-projects Slack channel with your idea or just that you're available to pair up. Project proposals are due on Gradescope a few weeks into the course. Please read more information about the projects . Certificate Those who complete the assignments and project will receive a certificate that can, for example, be displayed on LinkedIn. Time Commitment On average, expect to spend 5-10 hours per week on the course.","title":"Synchronous Online Course"},{"location":"spring2021/synchronous/#synchronous-online-course","text":"For those of you who signed up, in addition to the lecture and lab materials released publicly, there are some major extras: Slack workspace for learners, instructors, and teaching assistants Weekly graded assignment Capstone project reviewed by peers and staff Certificate of completion","title":"Synchronous Online Course"},{"location":"spring2021/synchronous/#how-do-i-know-if-i-am-in-this-course","text":"If you registered and received an email receipt from Stripe, you're in, and should have been added to our Slack workspace on February 1. Please email us if you have a Stripe receipt but aren't in our Slack.","title":"How do I know if I am in this course?"},{"location":"spring2021/synchronous/#teaching-assistants","text":"This course is only possible with the support of our amazing TAs: Head TA James Le runs Data Relations for Superb AI and contributes to Data Journalism for Snorkel AI, after getting an MS in Recommendation Systems at RIT. Daniel Cooper is a machine learning engineer at QuantumWork, SaaS for recruiters. Han Lee is a Senior Data Scientist at WalletHub. Prior to that, he worked on various DS, MLE, and quant roles. Previously, he co-managed TEFQX . Nadia Ahmed is a machine learning researcher with The Frontier Development Lab and Trillium Technologies in remote sensing for severe weather and flood events. Andrew Mendez is a Senior Machine Learning Engineer at Clarifai, developing large scale computer vision and machine learning systems for the public sector. Previously he was a ML Engineer at CACI. Vishnu Rachakonda is a Machine Learning Engineer at Tesseract Health, a retinal imaging company, where he builds machine learning models for workflow augmentation and diagnostics in on-device and cloud use cases. Chester Chen is the Director of Data Science Engineering at GoPro. He also founded the SF Big Analytics Meetup.","title":"Teaching Assistants"},{"location":"spring2021/synchronous/#schedule","text":"While we post lectures once a week starting February 1, the first four weeks are review lectures -- stuff you should already know from other courses. On March 1, we get to the Full Stack content, and you will begin doing weekly assignments, discussing in Slack, and thinking about their course project.","title":"Schedule"},{"location":"spring2021/synchronous/#logistics","text":"All learners, instructors, and TAs will be part of a Slack workspace. The Slack community is a crucial part of the course: a place to meet each other, post helpful links, share experiences, ask and answer questions. On Monday, we post the lecture and lab videos for you to watch. Post questions, ideas, articles in Slack as you view the materials. On Thursday, we go live on Zoom to discuss the posted questions and ideas. We have two 30-min slots: 9am and 6pm Pacific Time. We will send everyone a Google Calendar invite with the Zoom meeting information. You have until Friday night to finish the assignment via Gradescope, which will be graded by next Tuesday, so that you have prompt feedback. Labs are not graded and can be done on your own.","title":"Logistics"},{"location":"spring2021/synchronous/#projects","text":"The final project is the most important as well as the most fun part of the course. You can pair up or work individually. The project can involve any part of the full stack of deep learning, and should take you roughly 40 hours per person, over 5 weeks. Projects will be presented as five-minute videos and associated reports, and open sourcing the code is highly encouraged. All projects will be posted for peer and staff review. The best projects will be awarded and publicized by Full Stack Deep Learning. If you want to find a partner, please post in the #spring2021-projects Slack channel with your idea or just that you're available to pair up. Project proposals are due on Gradescope a few weeks into the course. Please read more information about the projects .","title":"Projects"},{"location":"spring2021/synchronous/#certificate","text":"Those who complete the assignments and project will receive a certificate that can, for example, be displayed on LinkedIn.","title":"Certificate"},{"location":"spring2021/synchronous/#time-commitment","text":"On average, expect to spend 5-10 hours per week on the course.","title":"Time Commitment"}]}